{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/12/24/hello-world/"},{"title":"안녕하세요hexo","text":"테스트 제목안녕하세요 hexo 새글을 쓴다 터미널에서 “hexo generate” 터미널에서 “hexo deploy” 블로그를 확인한다!","link":"/2019/12/24/%EC%95%88%EB%85%95%ED%95%98%EC%84%B8%EC%9A%94hexo/"},{"title":"주사위굴리기_14499번","text":"14499번 문제 풀이 주사위가 좌표면을 움직일때마다 윗면에 쓰여있는 숫자를 출력하는 문제이다. 1.규칙에 따라 주사위와 좌표면의 숫자들이 바뀌므로 바뀌는 주사위 면의 숫자들을 저장하고자 했다. &nbsp;&nbsp;&nbsp;따라서, 아래와 같이 dice라는 리스트를 주사위값 저장에 사용했다. 1dice = [0,0,0,0,0,0] #위0 아래1 동2 서3 남4 북5 순서로 값 2.주사위가 움직이는 지도가 어떻게 생겼는지를 알아야했다. &nbsp;&nbsp;&nbsp;따라서, 아래와 같이 입력받은 크기에 맞는 지도를 생성했다. 12345field = []for row in range(rows): li = list(map(int, input().split())) field.append(li) 3.주어지는 움직임대로 주사위를 움직이고 그에따라 지도의 값과 dice의 값을 바꿔주어야한다. 123456789101112131415161718192021for i in range(len(moves)): temp = 0 if moves[i] == 1: if position_y + 1 &lt; columns: position_y += 1 temp = dice[2] field_num = field[position_x][position_y] if field_num == 0: dice[2] = dice[0] dice[0] = dice[3] dice[3] = dice[1] dice[1] = temp field[position_x][position_y] = temp else: dice[2] = dice[0] dice[0] = dice[3] dice[3] = dice[1] dice[1] = field[position_x][position_y] field[position_x][position_y] = 0 else : continue 주사위가 특정 방향으로 움직이기전에, temp값에 바닥을 향할 주사위 값을 미리 저장해주었다. 그리고 주사위가 움직일때, 현재 좌표값들을 변경시켜준다. 이때 주의할점은 우리는 문제에서 값을 입력받을때 세로크기를 먼저받고 그 다음으로 가로 크기를 받는다는 점이다. 즉 동쪽으로 움직이면 y좌표가 늘어난다. 주사위가 굴렀다면, dice 리스트에 값들을 바꿔줘야한다. 위와같이 동쪽으로 굴렀다면, 위를 바라보던 주사위 면은 동쪽을 바라보게 될테고 서쪽을 바라보던 주사위면은 위를 바라보게 될 것이다. 이와 같이 주사위 면의 값들을 dice에 바꿔서 입력한다. 이 과정을 동,서,남,북에 나눠서 처리하고 그때마다 주사위 윗면의 값을 프린트해주면 된다. Full CodeFull Code","link":"/2019/12/27/%EC%A3%BC%EC%82%AC%EC%9C%84%EA%B5%B4%EB%A6%AC%EA%B8%B0-14499%EB%B2%88/"},{"title":"Centauri","text":"[Python] 백준1011번 풀이Fly me to the Alpha Centauri1.거리에 따른 이동 규칙을 찾는 문제였다. 마지막 거리1을 제외한 나머지 거리를 역으로 탐색하는 방법을 생각했지만 규칙성을 찾아서 해결하는 문제에 옳지 못한 방법이었다.&nbsp;&nbsp;&nbsp;타 게시판에서 힌트를 찾을 수 있었는데, 일정 거리를 지속적으로 늘려가다가 결국 마지막 1광년을 가기 위해 어느 시점부터 다시 지속적으로 움직이는 거리를 줄여야 한다는것이다. 2.그렇다면 어느 시점부터 다시 움직이는 거리를 줄이는 것일까?&nbsp;&nbsp;&nbsp;이에 대한 해답은 총 이동거리에 따라서 어떻게 움직여야하는지 표를 그려 파악할 수 있었다. 거리 이동경로 움직인 횟수 1 1 1 2 1 1 2 3 1 1 1 3 4 1 2 1 3 5 1 2 1 1 4 6 1 2 2 1 4 7 1 2 2 1 1 5 8 1 2 2 2 1 5 9 1 2 3 2 1 5 10 1 2 3 2 1 1 6 11 1 2 3 2 2 1 6 12 1 2 3 3 2 1 6 13 1 2 3 3 2 1 1 7 14 1 2 3 3 2 2 1 7 15 1 2 3 3 3 2 1 7 16 1 2 3 4 3 2 1 7 17 1 2 3 4 3 2 1 1 8 18 1 2 3 4 3 2 2 1 8 19 1 2 3 4 3 3 2 1 8 20 1 2 3 4 3 3 2 1 8 21 1 2 3 4 4 3 2 1 1 9 3.위의 표를 살펴보면 제곱수가 되는 K(볼드체가 표시되어 있는 2,3,4….) 를 기준으로 움직인 회수가 바뀌는것을 확인할 수 있다.&nbsp;&nbsp;&nbsp;다시말해, 우리가 움직여야하는 거리가 주어졌을 때 가장 가까운 제곱수로 나타낼 수 있는 값 K를 찾아야한다. 12345x, y = map(int, input().split())stand = 0if math.sqrt(y-x) - math.floor(math.sqrt(y-x)) &lt; 0.5: stand = math.floor(math.sqrt(y-x))else: stand = math.ceil(math.sqrt(y-x)) 총 움직여야하는 거리 y-x 에 대해&nbsp;&nbsp;&nbsp;sqrt값과 sqrt의 floor값 차이를 구해 이 값이 0.5 보다 작다면 기준이 되는 K값(위의 코드에서는 stand값)은 sqrt의 floor값이라고 할 수 있다.반대로, 0.5보다 크다면 K값은 sqrt의 ceil값이 될 것이다. 4.기준이 되는 값 K를 구했다면 K와 움직인 횟수와의 관계를 살펴봐야 한다.&nbsp;&nbsp;&nbsp;우리가 움직인 거리와 K의 제곱값을 비교하면 알 수 있다.&nbsp;&nbsp;&nbsp;만약, y-x가 K의 제곱보다 크다면 움직인 횟수는 K2이며&nbsp;&nbsp;&nbsp;y-x가 k의 제곱보다 작거나 같다면 움직인 횟수는 k2-1의 규칙을 따르는 것을 확인할 수 있다. 1234if y-x &gt; stand**2: print(stand*2)elif y-x &lt;= stand**2: print(stand*2-1) Full codeFull code","link":"/2019/12/30/Fly%20me%20to%20the%20Alpha%20Centauri/"},{"title":"부녀회장이될테야_2775번","text":"[Python] 백준 2775번 풀이부녀회장이 될테야1.처음 문제를 일고 떠오른것은 점화식 문제였다.&nbsp;&nbsp;&nbsp;점화식을 만들기 위해 몇층을 예로들어 직접 작성해보았다. 0층 : 1 2 3 4 5 6 7 8…1층 : 1 3 6 10 15 21 28…2층 : 1 4 10 20 25 56…3층 : 1 5 15 35 70…4층 : 1 6 21 56 몇개의 예를 들어 적어놓고 보니 계차수열의 형태를 나타낸다고 생각했다.하지만 층수가 높아질수록 계차수열에 계차수열이 더해지는 형태로 나타났고 이를 점화식으로 나타내는것은 무리가 따랐다.따라서, 문제에서 요구하는 층수와 호실수가 많지 않았으므로 전체에 대한 계산 결과를 저장하기로 결정했다. 123456789li = [[0]*14 for i in range(15)]for i in range(1,15): li[0][i-1] = i li[i][0] = 1for i in range(1,15): for j in range(1,14): li[i][j] = li[i-1][j] + li[i][j-1] 0층을 포함한 총 15층의 데이터를 저장하고 테스트 케이스로 입력받은 내용을 나타내는것으로 마무리했다. Full CodeFull Code","link":"/2020/01/01/%EB%B6%80%EB%85%80%ED%9A%8C%EC%9E%A5%EC%9D%B4%EB%90%A0%ED%85%8C%EC%95%BC-2775%EB%B2%88/"},{"title":"hexo블로그_댓글창만들기_icarus테마","text":"icarus테마 hexo블로그에 disqus 댓글창을 만들어보자icarus테마 블로그에 disqus댓글창을 만들고자 5시간의 삽질을 기록하는 이야기이다.1.disqus사이트에 접속해 아이디를 만들어아합니다. 2.홈 화면에서 get started를 누른다. 3.”I want to install Disqus on My site”를 누릅니다. 4.website이름을 만들고자하는 이름으로 입력하고 카테고리 언어를 설정한 후 create site를 클릭합니다. 5.여러 유료모델이 있지만 트래픽이 높지 않은 개인 블로그이므로(높을수도 있지만?!) Basic 모델아래에 Subscribe Now를 눌러줍니다. 6.어떤 플랫폼을 사용하는지 물어보지만 우리는 github.io를 사용하기때문에 제일 아래쪽의 I dont’ see my platform listed….을 눌러줍니다. 7.Universal Code install instructions창이 뜹니다. 8.해당 창에서 제일 위의 settings을 눌러줍니다. 9.새로 열리는 창의 두번째칸에 Shortname이 있습니다. Your website shortname is XXXXX라고 나옵니다. shortname을 기억하시면됩니다. 이제 github.io블로그를 꾸미는 터미널로 들어갑니다!테마마다 설정하는 법이 모두 다르기때문에 icarus테마를 기준으로 설명하겠습니다.icarus테마의 icarus테마 댓글창 설정법에 들어가면 _config파일만 수정해 적용하도록 나와있습니다. 1234comment: # Name of the comment plugin type: disqus shortname: &lt;9번에서 기억한 shortname&gt; 위와 같이 _config파일을 수정하고 적용을 시키고 테스트를 하기위해 서버를 실행시켰지만 터미널에서는 comment.shortname을 찾을 수 없다는 에러메시지였습니다.여기서부터 장장 5시간의 삽질이 시작됐습니다.저의 에디터에서는 저렇게 수정한것처럼 보였습니다…하지만 다른 커뮤니티에 문의하기 위해 comment 아래 부분을 복사해서 메모장에 붙여넣기 해보니 실제로 입력되어있던 내용은 1234comment: # Name of the comment plugin type: disqus shortname: &lt;shortname&gt; 였습니다. OMG…comment.shortname을 찾을 수 없다는 에러메시지가 나오는게 당연한 상황이었습니다저걸 찾기까지 무려 5시간이 걸리다니… 에디터에서는 바르게 “보이길래” 의심도 못했습니다.(역시 갓모장…)결국 footer수정… disqus.ejs파일 수정…등등 삽질을 끝낼 수 있었습니다. 저와같은 오류가 뜬다면 삽질을 막기위해 _config 문서를 수정하시고 메모장에 복사 붙여넣기를 해보세요!결론적으로 _config.yml 파일의 comment 내용을 다음과 같이 수정하시면 올바르게 적용됩니다! 1234comment: # Name of the comment plugin type: disqus shortname: &lt;9번에서 기억한 shortname&gt; disqus파일은 수정하지 않으셔도되고 위와같이 수정한 후 1hexo server -p &lt;포트번호&gt; ## 로컬에서 확인하기 로컬에서 확인한 후 12hexo generatehexo deploy 로 배포하시면 됩니다.","link":"/2020/01/03/hexo%EB%B8%94%EB%A1%9C%EA%B7%B8-%EB%8C%93%EA%B8%80%EC%B0%BD%EB%A7%8C%EB%93%A4%EA%B8%B0-icarus%ED%85%8C%EB%A7%88/"},{"title":"택시기하학_3053번","text":"[Python] 백준 3053번 풀이택시 기하학1.택시 기하학에서 원이 어떻게 정의되는지 이해하면 매우 간단한 문제이다.&nbsp;&nbsp;&nbsp; 아래의 그림을 통해 택시 기하학에서의 원을 살펴볼 수 있다. 직교 좌표계에서 같은 거리에 있는 점들의 집합을 원이라고 정의하므로 위와같이 나타낼 수 있다. 따라서 빨간 점들을 살펴보면 정사각형을 이루는것을 알 수 있고 정사각형의 넓이는 반지름 r이 주어졌을때 다음과 같이 구할 수 있다. 1k = 2*(r**2) ## 피타고라스 정리에 의해 c^2 = a^2 + b^2 = a^2 + a^2 = 2*a^2 Full CodeFull Code","link":"/2020/01/03/%ED%83%9D%EC%8B%9C%EA%B8%B0%ED%95%98%ED%95%99-3053%EB%B2%88/"},{"title":"블랙잭_2798번","text":"[Python] 백준 2798번 풀이블랙잭1.모든 조합의 경우의 수를 따지는 문제이다.파이썬에서는 itertools의 combinations를 이용해 조합을 따질 수 있다. itertools.combinations에서 확인할 수 있다. 2.combinations는 combinations object를 리턴하는데 이를 for루프에 돌려 하나의 조합을 확인하게되면 튜플 을 리턴한다.예를들어, 1234li = [3,4,5]for i in itertools.combinations(li,2): print(i)## &gt;&gt; (3,4) , (3,5) , (4,5) 를 출력한다. 3.따라서 입력받은 N개의 숫자 리스트로부터 모든 조합을 따지면서 그 값이 상한값을 넘지않는 경우 새로운 변수에 입력해주면 된다. 1234567com_li = 0for i in itertools.combinations(li,3): s = sum(i) if s &gt; com_li and s &lt;= m: com_li = s else: pass Full CodeFull Code","link":"/2020/01/03/%EB%B8%94%EB%9E%99%EC%9E%AD-2798%EB%B2%88/"},{"title":"Golang_정리하기-1","text":"[Go tutorial] Go 정리하기 - 1Go-tour 참조 익스포트 Go 에서는 첫 문자가 대문자로 시작하면 특정 패키지를 사용하는 곳에서 접근할 수 있는 exported name이 됨 함수 Go 는 코드를 왼쪽부터 자연스럽게 읽기 위해 매개변수 타입은 변수명 뒤에 명시. 두 개 이상의 매개변수가 같은 타입일 때, 같은 타입을 취하는 마지막 매개변수에만 타입을 지정할 수 있음. 12345x int, y int#를 아래와 같이도 사용 가능x, y int 하나의 함수는 여러 개의 결과를 반환할 수 있음. 123func swap(x, y string) (string, string) { return y, x} 변수 var 을 통해 변수를 선언. 변수 선언과 함꼐 초기화 가능, 초기화를 하는 경우 타입 생략 가능. 이 경우, 초기화하고자 하는 값에 따라 타입이 결정됨. 1var a, b, c int = 1, 2, 3 함수 내에서 := 을 사용하면 var와 명시적인 타입을 생략할 수 있음. 상수 const 키워드와 함께 변수처럼 선언. 상수는 문자, 문자열, boolean, 숫자 타입 중의 하나가 될 수 있음 숫자형 상수는 정밀한 값을 표현할 수 있음. Go-tour 사이트에서 제공하는 예제를 이해하기 위해서는 Go의 &lt;&lt; 와 &gt;&gt; 연산자의 이해가 필요하다. &lt;&lt;, &gt;&gt; 연산자에 대해 &lt;&lt; 와 &gt;&gt; 연산자는 비트 이동 연산자로 Left shift, Right shift 라고 함. Left shift 는 현재 값의 비트를 주어진 값만큼 왼쪽으로 Right shift는 현재 값의 비트를 주어진 값만큼 오른쪽으로 옮깁니다.1234567891011const (a = 1b = a &lt;&lt; 1)func main() { fmt.Println(a) ### 1 출력 fmt.Printf(\"%08b\\n\", a) ### 00000001 출력 fmt.Println(b) ### 2 출력 fmt.Printf(\"%08b\", b) ### 00000010 출력} 반복문 for Go 에는 반복문으로 for 하나만 존재. 조건문만 표현해서 루프를 표현 가능. 1234567func main() { sum := 1 for sum &lt; 1000 { sum += sum } fmt.Println(sum)} 위와 같이 표현하면 while을 사용하듯 for을 사용할 수 있다. 조건문을 생략하는것으로 무한 루프를 만들 수 있다. 1234func main() { for { }} 조건문 if 반복문과 마찬가지로 실행문을 위한 { } 만 필요. for 처럼 조건문 앞에 문장을 실행할 수 있음. 123456func pow(x, n, lim float64) float64 { if v := math.Pow(x, n); v &lt; lim { return v } return lim} 위의 예를 보면, v &lt; lim 을 통한 조건문 앞에 v := math.Pow(x,n) 을 실행했다. 이렇게 선언된 변수(위에서는 v)는 if 안쪽 범위에서만(else 블럭 안에서도 가능) 사용할 수 있다. 연습 : 루프와 함수 &gt;&gt;&gt; solution","link":"/2020/01/06/Golang-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0-1/"},{"title":"Golang_정리하기_2","text":"[Go tutorial] Go 정리하기 - 2Go-tour 참조구름edu 참조 구조체(Structs) structs는 데이터들의 조합 type선언으로 이름을 지정할 수 있음 “객체이름 := 구조체이름{저장할값}”으로 입력해 선언과 동시에 초기화할 수 있다. 구조체에 속한 필드(데이터)는 dot(.)으로 접근 1234567891011121314151617181920212223242526type person struct { name string age int contact string}func main() { var p1 = person{} fmt.Println(p1) p1.name = \"kim\" p1.age = 25 p1.contact = \"01000000000\" fmt.Println(p1) p2 := person{\"nam\", 31, \"01022220000\"} // 필드 이름을 생력할 시 순서대로 저장함 fmt.Println(p2) p3 := person{contact: \"01011110000\", name: \"park\", age: 23} // 필드 이름을 명시할 시 순서와 상관 없이 저장할 수 있음 fmt.Println(p3) p3.name = \"ryu\" //필드에 저장된 값을 수정할 수 있음 fmt.Println(p3) fmt.Println(p3.contact) //필드 값의 개별 접근도 가능함} 포인터 포인터 연산은 불가능 포인터를 이용한 간접 접근은 실제 구조체에도 영향을 끼침 “&amp;” 을 매개변수 앞에 붙여 pass by reference 를 통해 값이 저장된 주소에 직접 접근할 수 있음. 구조체 포인터를 생성하는 방법은 두 가지가 있다. ‘new(구조체이름)’ 을 사용하여 객체를 생성 구조체 이름 앞에 &amp; 붙이기 12345678910111213141516171819type Person struct { Name string}func main() { c := new(Person) // returns pointer c.Name = \"Catherine\" fmt.Println(c.Name) // prints: Catherine d := *c d.Name = \"Daniel\" fmt.Println(d.Name) // prints: Daniel i := &amp;d i.Name = \"Ines\" fmt.Println(c.Name) // prints: Catherine fmt.Println(d.Name) // prints: Ines fmt.Println(i.Name) // prints: Ines} 슬라이스(slice) 슬라이스는 참조 타입이다. 따라서, 슬라이스를 복사한다는 것은 같은 주소를 참조한다는 것이다. 복사한 슬라이스의 값을 바꾸면 참조하는 슬라이스의 해당 값도 바뀌게 된다. 슬라이스 선언 var a []int = []int{1,2,3,4} 와 같이 선언과 함께 초기화 make() 함수를 이용. make(슬라이스 타입, 슬라이스 길이, 슬라이스 용량) ex) s:=make([]int,3,3) append() 함수를 통해 슬라이스에 데이터를 추가할 수 있다. 슬라이스에 슬라이스를 추가하기 위해서는 슬라이스 뒤에 “…”을 입력 12345678910func main() { sliceA := []int{1, 2, 3} sliceB := []int{4, 5, 6} sliceA = append(sliceA, sliceB...) //sliceA = append(sliceA, 4, 5, 6) fmt.Println(sliceA) fmt.Println(sliceB)} copy(붙여넣을 슬라이스, 복사할 슬라이스) 를 통해 한 슬라이스를 다른 슬라이스로 복사할 수 있다. 12345678910111213141516func main() { c := make([]int, 0, 3) //용량이 3이고 길이가0인 정수형 슬라이스 선언 c = append(c, 1, 2, 3, 4, 5, 6, 7) fmt.Println(len(c), cap(c)) l := c[1:3] //인덱스 1요소부터 2요소까지 복사 fmt.Println(l) l = c[2:] //인덱스 2요소부터 끝까지 복사 fmt.Println(l) l[0] = 6 fmt.Println(c) //슬라이스 l의 값을 바꿨는데 c의 값도 바뀜//값을 복사해온 것이 아니라 기존 슬라이스 주솟값을 참조} 위의 예제를 실행해보면 slice c의 값도 l 에 의해 바뀌는것을 확인할 수 있다. 왜냐하면, 슬라이스는 배열과 다르게 슬라이스 자체가 참조하고 있는 주소값을 같이 참조하기 때문이다. 슬라이스 순회(iterates) 슬라이스는 for 반복문에서 range를 통해 순회할 수 있다. 이때, index와 value를 순회하며 필요치 않은 값은 “_” 를 이용해 무시할 수 있다. 연습 : 슬라이스 - 문제 연습 : 슬라이스 - solution","link":"/2020/01/07/Golang-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0-2/"},{"title":"Golang_정리하기_3","text":"[Go tutorial] Go 정리하기 - 3Go-tour 참조구름edu 참조 맵(map) 키와 value의 조합을 나타내기 위한것으로 파이썬의 딕셔너리를 생각하자. var 맵이름 map[key자료형]value자료형 으로 선언 var a map[int]string :: key가 int이고 value가 string인 맵 a 선언만 하고 초기화하지 않는다면 Nil map 이다. 데이터 추가 or 갱신 :: 맵이름[key] = value 를 이용 데이터 삭제 :: delete(맵이름,key) 를 이용 123456789101112131415161718192021func main() { //지역번호와 지역 저장 var m = make(map[string]string) m[\"02\"] = \"서울특별시\" m[\"031\"] = \"경기도\" m[\"032\"] = \"충청남도\" m[\"053\"] = \"대구광역시\" fmt.Println(m) //동일한 key값으로 value값을 저장하면 갱신이 된다 m[\"032\"] = \"인천\" fmt.Println(m) //m에 있는 \"031\"key의 value와 함께 삭제 delete(m, \"031\") fmt.Println(m)} key체크, value 확인 “맵이름[key]” 는 value와 키가 존재하는지 여부를 반환한다. 존재하지 않는 키값이라면 자료형에따라 0 혹은 “” 를 반환 해당 키가 존재하는지 여부에 따라 true/false 반환 12345678910111213141516func main() { m := make(map[string]int) m[\"Answer\"] = 42 fmt.Println(\"The value:\", m[\"Answer\"]) ## print : The value: 42 m[\"Answer\"] = 48 fmt.Println(\"The value:\", m[\"Answer\"]) ## print : The value: 48 delete(m, \"Answer\") fmt.Println(\"The value:\", m[\"Answer\"]) ## print : The value: 0 v, ok := m[\"Answer\"] fmt.Println(\"The value:\", v, \"Present?\", ok) ## print : The value: 0 Present? false} 연습 : 맵 - 문제 연습 : 맵 - solution 함수 클로져 클로져는 함수 안에서 익명 함수를 정의해 바깥쪽 함수에서 선언한 변수에도 접근할 수 있는 함수를 뜻한다. 1234567891011121314151617func adder() func(int) int { sum := 0 return func(x int) int { sum += x return sum }}func main() { pos, neg := adder(), adder() for i := 0; i &lt; 10; i++ { fmt.Println( pos(i), neg(-2*i), ) }} 위의 예에서 adder() 함수는 지역 변수 sum 을 초기화하고 입력받는 x를 더해주는 익명 함수를 반환합니다. for문에서 함수가 실행될때마다 지역 변수가 초기화되지않고 더해주는 익명함수만을 통해 값이 지속적으로 증가/감소 하는것을 확인할 수 있습니다. 연습 : 피보나치 클로져 - 문제 연습 : 피보나치 클로져 - solution","link":"/2020/01/08/Golang-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0-3/"},{"title":"분해합_2231번","text":"[Python, Go] 백준 2231번 풀이분해합 자연수 N의 생성자를 찾기 위해 1부터 N이전까지의 수를 모두 탐색한다. N이 1일때는 생성자가 존재하지 않으므로 0을 출력한다. i 를 키워나가면서 가장 작은 생성자를 찾으면 멈추고, N-1까지 탐색했을 때 결과값이 존재하지 않다면 0을 출력한다. i 의 각 자리수를 sum에 더해줄때, 각 자리수를 for문으로 돌려서 더해주었음. —&gt; 10으로 나누고 나머지를 사용하는 방법도있다. Python 풀이 1234567891011121314n = int(input())if n==1: print(0)for i in range(1,n): sum = i for j in str(i): sum += int(j) if sum == n: print(i) break elif i == n-1 : print(0) Go 풀이 Python과 같은 방법을 사용했는데 Go에서는 int를 스트링으로 바꿔주기 위해 “strconv”를 사용하였다. strconv.Itoa를 통해 int를 string으로 변환 strconv.ParseInt를 통해 string을 int로 바꿔주었다. 1234567891011121314151617181920212223242526272829package mainimport ( \"fmt\" \"strconv\")func main() { var n int fmt.Scanf(\"%d\", &amp;n) if n == 1 { fmt.Printf(\"%d\", 0) } for i := 1; i &lt; n; i++ { sum := i for _, j := range strconv.Itoa(sum) { k, _ := strconv.ParseInt(string(j), 10, 8) sum += int(k) } if sum == n { fmt.Printf(\"%d\", i) break } else if i == n-1 { fmt.Printf(\"%d\", 0) } }} Full CodeFull Code - Python Full Code - Go","link":"/2020/01/08/%EB%B6%84%ED%95%B4%ED%95%A9-2231%EB%B2%88/"},{"title":"Golang_정리하기_4","text":"[Go tutorial] Go 정리하기 - 4Go-tour 참조구름edu 참조 스위치 (switch) case의 라벨과 일치하는경우를 찾아 실행 어느것에도 맞지않다면 default문으로 실행 가능 break를 따로 입력하지 않아도 해당되는 case문만 실행된다. 스위치의 각 조건은 위에서 아래로 평가한다. 참인 case를 찾으면 평가를 마침. switch 전달 인자 태그 표현식 전달되는 인자 없이 case에 표현식 사용 가능 1234567891011121314151617func main() { var a, b int fmt.Print(\"정수 a와 b를 입력하시오:\") fmt.Scanln(&amp;a, &amp;b) switch { case a &gt; b: fmt.Println(\"a가 b보다 큽니다.\") case a &lt; b: fmt.Println(\"a가 b보다 작습니다.\") case a == b: fmt.Println(\"a와 b가 같습니다.\") default: fmt.Println(\"모르겠어요.\") }} 메소드와 인터페이스 메소드란 특정 속성들(구조체)을 이용해 기능을 수행하기 위해 만들어진 함수이다. 선언 “func (매개변수이름 구조체이름) 메소드이름() 반환형 {}” 형식으로 선언 매개변수 이름은 메소드 내에서 매개변수처럼 사용 123456789101112type Vertex struct { X, Y float64}func (v *Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y)}func main() { v := &amp;Vertex{3, 4} fmt.Println(v.Abs()) ### print : 5} 위의 예에서는 메소드에 구조체를 붙였다. 뿐만 아니라 메소드에는 아무 타입이나 붙일 수 있다. 12345678910111213type MyFloats float64func (f MyFloats) Abs() float64 { if f &lt; 0 { return float64(-f) } return float64(f)}func main() { k := MyFloats(-math.Sqrt2) fmt.Println(k.Abs()) ### print : 1.4142135623730951} 위의 예에서는 float타입을 메소드에 붙였다. 인터페이스 구조체가 변수를 묶어놓은 것이라면, 인터페이스는 메소드를 모아놓은 것이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package mainimport ( \"fmt\" \"math\")type geometry interface { area() float64 perimeter() float64 }type Rect struct { width, height float64}type Circle struct { radius float64}func (r Rect) area() float64 { return r.width * r.height}func (c Circle) area() float64 { return math.Pi * c.radius * c.radius}func (r Rect) perimeter() float64 { return 2 * (r.width + r.height)}func (c Circle) perimeter() float64 { .Pi * c.radius}func main() { r1 := Rect{10, 20} c1 := Circle{10} r2 := Rect{12, 14} c2 := Circle{5} printMeasure(r1, c1, r2, c2)}func printMeasure(m ...geometry) { for _, val := range m { fmt.Println(val) fmt.Println(val.area()) fmt.Println(val.perimeter()) }} 위의 예를 보면 인터페이스 안에 메소드가 들어가있다. 또한, 같은 이름의 메소드가 두개씩 있다. 이름이 동일하게 선언되어도 메소드가 전달받는 구조체가 다르기 때문에 괜찮은것이다. 메인함수에서 구조체를 초기화하고 for문을 통해 각 구조체를 전달받은 메소드에 대해 결과값을 출력하고 있다. 빈 인터페이스 인터페이스는 어떠한 타입도 담을 수 있는 dynamic type 이다. 인터페이스는 매개변수로 사용할 수 있다. 인터페이스는 내용을 따로 선언하지 않아도 형으로 사용할 수 있다. 하나의 변수를 형이 다른 형태로 저장해 출력한다고 생각한다면, 매개변수형에 따른 2개 이상의 함수를 만들어야한다. 하지만 빈 인터페이스를 사용한다면, 어떤 형도 담을 수 있기 때문에 편하게 사용이 가능하게된다. 1234567891011121314151617package mainimport \"fmt\"func printVal(i interface{}) { fmt.Println(i)}func main() { var x interface{} //빈 인터페이스 선언 x = 1 printVal(x) x = \"test\" printVal(x)} 위의 예에서 x의 값에따라 다르게 형을 선언하지 않고 출력하는것을 확인할 수 있다.","link":"/2020/01/09/Golang-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0-4/"},{"title":"스택_10828번","text":"[Python, Go] 백준 10828번 풀이스택 주어지는 명령에 따라 스택을 처리하는 문제이다. 파이썬은 큐나 스택을 지원하는 라이브러리가 있지만 사용하지 않고 리스트를 통해 구현했으며, Go는 슬라이스를 통해 구현했다. 스택을 초기화할 필요가 있는 문제가 아니었으므로 하나의 파이썬의 경우 하나의 for문안에서 처리하고 Go는 각각의 명령에 따른 함수를 만들어 사용했다. Python 풀이 1234567891011121314151617181920212223242526272829303132333435import sysn = int(sys.stdin.readline())stack = []# n = int(input())for i in range(n): order = sys.stdin.readline().rstrip() # print(order) if len(order.split()) == 2: stack.insert(0,int(order.split()[1])) else: if order == \"pop\": if stack == []: print(-1) else: print(stack[0]) stack.pop(0) elif order == \"size\": print(len(stack)) elif order == \"empty\": if stack == []: print(1) else: print(0) # top else: if not stack == []: print(stack[0]) else: print(-1) Go를 통해 슬라이스를 다룰때 파이썬의 pop과 같은 내장함수가 없다. 따라서, 슬라이스에서 하나를 제거할 때 슬라이스의 길이가 1인경우와 1이 아닌경우를 나눠서 생각했다. 슬라이스의 길이가 1인경우, 남아있는 하나를 제거하고 empty slice를 만들기 위해 nil 처리 슬라이스의 길이가 1이 아닌 경우, 슬라이스의 가장 끝의 하나를 제거하면 되므로 슬라이스의 길이 L 을 구하고 L-1 까지의 슬라이스만을 가져간다. Go 풀이 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package mainimport ( \"bufio\" \"fmt\" \"os\" \"strconv\" \"strings\")var s []intfunc s_push(x int) { s = append(s, x)}func s_pop() { if s == nil { fmt.Printf(\"%d\\n\", -1) } else if len(s) == 1 { top := s[0] fmt.Printf(\"%d\\n\", top) s = nil } else { l := len(s) top := s[l-1] fmt.Printf(\"%d\\n\", top) s = s[:l-1] }}func s_size() { l := len(s) fmt.Printf(\"%d\\n\", l)}func s_empty() { if s == nil { fmt.Printf(\"%d\\n\", 1) } else { fmt.Printf(\"%d\\n\", 0) }}func s_top() { if s == nil { fmt.Printf(\"%d\\n\", -1) } else { l := len(s) top := s[l-1] fmt.Printf(\"%d\\n\", top) }}func main() { var n int inputReader := bufio.NewReader(os.Stdin) fmt.Scanf(\"%d\", &amp;n) for i := 0; i &lt; n+1; i++ { input, _ := inputReader.ReadString('\\n') order := strings.Split(input, \" \") action := strings.TrimSpace(order[0]) if len(order) == 2 { n := strings.TrimSpace(order[1]) num, _ := strconv.Atoi(n) s_push(num) } else { switch action { case \"pop\": s_pop() case \"size\": s_size() case \"empty\": s_empty() case \"top\": s_top() } } }} Full CodeFull Code - Python Full Code - Go","link":"/2020/01/15/%EC%8A%A4%ED%83%9D-10828%EB%B2%88/"},{"title":"for문에서_string_사용하기","text":"[Go] string(문자열)을 for문으로 출력, 이용하기 Go로 문제를 풀 때, 문자열을 for문으로 돌려야 하는 경우가 자주 발생했다. 파이썬같은 경우, 문자열을 다음과 같이 입력하면 문자열 그대로 출력된다. 12for i in \"CAT\": print(i) #print : C A T Go도 문자열을 for문에 사용할 수는 있지만, 파이썬과 달리 Go는 인덱스와 포인터 값을 반환한다. 123456789101112131415package mainimport \"fmt\"func main() { str := \"CAT\" for _, r := range str { c := string(r) fmt.Println(c) } fmt.Println() for i, r := range str { fmt.Println(i, r, string(r)) }} 위의 코드를 실행시켜보면, 다음과 같이 출력된다.CAT 0 67 C1 65 A2 84 T 위와같이, Go에서는 for문에서 문자열이 아닌 포인터값(int)을 반환한다. 따라서 문자열을 이용하고 싶을 경우, 새로운 변수를 만들고 이에 string()함수를 통해 초기화 시켜주어야 한다. 위의 예처럼, 반환받은 r값을 새로운 변수 c := string(r) 로 초기화 시켜서 사용.","link":"/2020/01/15/for%EB%AC%B8%EC%97%90%EC%84%9C-string-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/"},{"title":"영화감독_숌_1436번","text":"[Python, Go] 백준 1436번 풀이영화감독 숌 숫자 6이 연속해서 3번 나타나는 수를 구하는 문제이다. 666, 1666, … , 5666, 의 다음은 6660, 6661, … 이다 666 부터 시작해서 수를 1씩 늘려가면서 모두 확인한다. 해당 수를 string으로 바꾸고 for문을 통해 6이 나왔을때 그 다음의 수도 6인지 그리고 그 다음 숫자도 6인지 확인하는 check 변수를 사용했다. 만약 check가 3이되면 해당 숫자는 6이 연속해서 3번 들어갔으므로 count를 1 증가시키고 주어진 목표만큼 count가 증가하면 종료, 출력했다. 123456789101112131415161718192021n = int(input())name = 666count = 0while True: if count == n: print(name-1) break else: check = 0 for i in str(name): if i == \"6\": check += 1 if check == 3: count += 1 continue else: pass else: check = 0 name += 1 Go도 같은 방식의 풀이지만, string을 for문에 돌릴때 반환되는 값은 포인터 값이므로 string(x) 로 바꿔주는것만 주의 참조 string을 int로 바꿀때 strconv.ParseInt 를 사용했으나 strconv.Atoi를 사용해도됨 1234567891011121314151617181920212223242526272829303132333435package mainimport ( \"fmt\" \"strconv\")func main() { var n int count := 0 name := 666 fmt.Scanf(\"%d\", &amp;n) for { if count == n { fmt.Printf(\"%d\", name-1) break } else { check := 0 for _, k := range strconv.Itoa(name) { v, _ := strconv.ParseInt(string(k), 10, 8) if v == 6 { check += 1 switch check { case 3: count += 1 } } else { check = 0 } } } name += 1 }} check 값이 3인지 확인할때 if문을 사용해도 괜찮지만 switch문을 사용해보았다. Full CodeFull Code - Python Full Code - Go","link":"/2020/01/15/%EC%98%81%ED%99%94%EA%B0%90%EB%8F%85-%EC%88%8C-1436%EB%B2%88/"},{"title":"괄호_9012번","text":"[Python, Go] 백준 9012번 풀이괄호 괄호의 쌍이 맞게 이루어져있는지 찾아내는 문제이다. “(“괄호와 “)” 괄호의 개수를 세는것에서 끝나면 안된다. 스택에 “(“ 가 들어왔을때 저장하고 “)” 와 만나면 스택에서 하나를 제거하는 방법으로 쌍을 맞춰주었다. 가장 먼저 들어오는 문자열이 “)” 이라면 쌍이 맞을 수 없으므로 NO 를 리턴하며, 문자열 모두를 파악한 후 stack이 비어있다면 YES 그렇지 않다면 NO 이다 Python 풀이 1234567891011121314151617181920212223242526import sysn = int(sys.stdin.readline())def vps(li): stack = [] if li[0] == \")\": return \"NO\" else: for s in li: if s == \"(\": stack.append(s) elif s == \")\" and stack != []: stack.pop() else: return \"NO\" if stack == []: return \"YES\" else: return \"NO\"for i in range(n): order = list(sys.stdin.readline().rstrip()) print(vps(order)) Go 풀이 Go 를 사용할때는 파이썬의 pop과같은 내장함수가 없으므로 “)”를 만났을때 슬라이스의 길이가 1인 경우와 그렇지 않은 경우를 나눠서 판단했다. func vps(x string) string { var stack []string for idx, ch := range x { s := string(ch) if idx == 0 &amp;&amp; s == \")\" { return \"NO\" } else { if s == \"(\" { stack = append(stack, s) } else if s == \")\" &amp;&amp; stack != nil { length := len(stack) if length == 1 { stack = nil } else { stack = stack[:length-1] } } else { return \"NO\" } } } if stack == nil { return \"YES\" } else { return \"NO\" } } Full CodeFull Code - Python Full Code - Go","link":"/2020/01/16/%EA%B4%84%ED%98%B8-9012%EB%B2%88/"},{"title":"균형잡힌_세상_4949번","text":"[Python, Go] 백준 4949번 풀이균형잡힌 세상 괄호문제 의 연장이다. 소괄호뿐만 아니라 대괄호까지 포함해 문장에서 괄호가 올바르게 되어져있는지 확인해야한다. 괄호안에 있는 괄호의 짝도 맞아야 yes를 출력할 수 있다. 예를들어 ( [ ) ] 와 같은 경우 하나의 괄호 안에 올바른 짝의 괄호가 들어가지 못했으므로 no 를 출력해야 한다. 정규표현식을 사용해 따라가야될 괄호들을 뺀 문자들은 pass 시켜주는 방법을 사용했다. 이외에는 괄호문제와같이 스텍에 ( 와 [ 를 담아서 사용했다. 단 ) 와 ] 를 만났을 때, 스텍의 마지막 요소가 짝이되는 괄호인지를 판별해줘야 한다. Python 풀이 12345678910111213141516171819202122232425262728293031323334def vps(line): stack = [] if line[0] == \")\" or line[0] == \"]\": # print(\"1\") return \"no\" else: pass for s in line: if bool(com.match(s)) == True: pass elif s == \" \" or s == \".\": pass elif s == \"(\" or s == \"[\": stack.append(s) # print(stack) elif s == \")\" and stack != []: if stack[-1] == \"(\": stack.pop() else: # print(\"2\") return \"no\" elif s == \"]\" and stack != []: if stack[-1] == \"[\": stack.pop() else: # print(\"3\") return \"no\" else: # print(\"4\") return \"no\" if stack == []: return \"yes\" else: # print(\"5\") return \"no\" Go 풀이 123456789101112131415161718192021222324252627282930313233343536373839404142434445func vps(x string) string { var stack []string // fmt.Println(string(x[0])) // return x if string(x[0]) == \")\" || string(x[0]) == \"]\" { return \"no\" } for _, ch := range x { s := string(ch) match, _ := regexp.MatchString(\"[a-zA-Z]\", s) if match == true { } else if s == \" \" || s == \".\" { } else if s == \"(\" || s == \"[\" { stack = append(stack, s) } else if s == \")\" &amp;&amp; stack != nil { if stack[len(stack)-1] == \"(\" { if len(stack) == 1 { stack = nil } else { stack = stack[:len(stack)-1] } } else { return \"no\" } } else if s == \"]\" &amp;&amp; stack != nil { if stack[len(stack)-1] == \"[\" { if len(stack) == 1 { stack = nil } else { stack = stack[:len(stack)-1] } } else { return \"no\" } } else { return \"no\" } } if stack == nil { return \"yes\" } else { return \"no\" }} Go로 풀이할때, 조건이 많을때는 switch가 더 좋기때문에 switch를 사용했다면 더 좋았을것같다. Full CodeFull Code - Python Full Code - Go","link":"/2020/01/19/%EA%B7%A0%ED%98%95%EC%9E%A1%ED%9E%8C-%EC%84%B8%EC%83%81-4949%EB%B2%88/"},{"title":"시간복잡도_big-O","text":"시간 복잡도(big-O)코딩 인터뷰 완전 분석(저 : 게일 라크만 맥도웰) 참조 실행 시간을 나타내기 위해 사용되는 개념이 시간 복잡도이다. 변수 N을 통해 O(logN), O(NlogN),… 등과 같이 표현하기도 하지만 이외에도 다양한 변수가 포함될 수 있다. 예를 들어, 너비가 w이고 높이가 h인 울타리를 칠한다고 할 때 소요 시간은 O(wh)로 표현할 수 있고 p번 덧칠한다면 O(pwh)로 표현할 수 있다. 상수항과 지배적이지 않은 항은 무시하자. big-O는 단순히 증가하는 비율을 나타내는 개념으로 수행 시간에 지배적이지 않은 항은 무시할 수 있다. 두 개의 중첩되지 않은 루프로 이루어진 코드의 수행 시간을 O(2N) 과 같이 표현할 수도 있지만 이는 결국 O(N)과 같은 의미이다. 마찬가지로, O(N^2 + N)과 같은 수행 시간이 있을 때, N^2이 지배적인 항이므로(N에 비해 빠르게 증가하기 떄문) O(N^2 + N) 은 O(N^2) 이 된다. 즉, big-O 는 수행 시간이 어떻게 변화하는지를 표현해주는 도구이다. 덧셈? 곱셈?? 두 단계로 이루어진 알고리즘이 있을 경우, 수행 시간을 어떤 경우에는 더하고 혹은 곱해야 하는걸까? 1234567### 덧셈 수행 시간 : O(A+B)for (int a : arrA) { print(a);}for (int b : arrB) { print(b);} 위와 같이 A의 일을 모두 끝마친 후 B의 일을 시행하는 형태라면 A와 B의 수행 시간을 더해야 한다. 123456### 곱셈 수행 시간 : O(A*B)for (int a : arrA) { for (int b : arrB) { print(a + \",\" + b); }} 위와 같이 A의 일을 할 때마다 B의 일을 시행하는 형태라면 A와 B의 수행 시간을 곱해야 한다. log N 수행 시간 logN 수행 시간이 어떻게 나오는지 알기 위해 이진 탐색을 생각해보자. 이진 탐색은 N개의 정렬된 원소가 들어있는 배열에서 원소 x를 찾을 때, 배열의 중간값과 x의 값을 비교하여 배열의 부분을 재탐색하는 것이다. 예를 들어, 16개의 원소를 가진 배열을 생각해보자. N = 16 이진 탐색을 통해 각 단계별로 탐색해야 하는 원소의 개수는 다음과 같다. 12345N = 16 # 처음N = 8 # 나누기 2N = 4 # 나누기 2N = 2 # 나누기 2N = 1 # 나누기 2, 찾고자하는 원소 x를 찾았다. 즉, 총 수행 시간은 N을 절반씩 나누는 과정을 몇단계 거쳐서 1이 되는지에 따라서 결정된다. 위의 경우를 반대로 생각해보자. 1에서 16으로 증가하려면 1에서 2를 몇번 곱해야 할까? 12345N = 1 # 처음N = 2 # 곱하기 2N = 4 # 곱하기 2N = 8 # 곱하기 2N = 16 # 곱하기 2 다시 말해, 절반씩 나누는 과정(나누기2)을 1에서 2를 곱해가는 과정으로도 표현할 수 있다는 뜻이다. 이는, 2를 k번 곱해서 N이 된다고 말할 수 있으며 다음과 같은 수식으로 나타낼 수 있다. 12^k = N 위 수식을 만족하는 k는 무엇인가? k를 찾기 위해 양 변에 밑이 2인 로그를 취해주자. 1234### &lt;밑이 2인 로그입니다.&gt;---&gt; log(2^k) = logN---&gt; k * log2 = logN---&gt; k = logN (∵ log2 = 1) big-O 에서는 로그의 밑을 고려할 필요가 없다. 위에서 밑이 10인 로그를 취했다 할지라도 이는 상수항일 뿐이다. 시작할 때 말했듯이 big-O 에서 상수항은 무시할 수 있다. 재귀함수의 수행 시간 다음과 같은 재귀함수의 수행 시간은 어떻게 될까? 123456int f(int n) { if (n&lt;=1) { return 1; } return f(n-1) + f(n-1);} 함수 f가 두번 호출되는것을 보고 O(N^2)이라고 생각할 수 있지만 틀렸다. f(4) 일때 위와 같이 f(3)을 두번 호출하고 f(3)은 f(2)를 거쳐 f(1)까지 호출한다. 위와 같이 두 개의 자식 노트를 가진 경우 총 호출 횟수는 얼마인가? 호출 횟수를 표로 나타내보자. 깊이 노드의 개수 2^N 표현 0 1 20 1 2 21 2 4 22 3 8 23 4 16 24 따라서, 전체 노드의 개수는 20 + 21 + 22 + 23 + … + 2N = 2N+1 - 1 이다. 즉, 위와 같은 재귀 함수의 수행 시간은 O(2N)이 된다. 보통 다수의 호출로 이루어진 재귀 함수의 수행 시간은 O(분기깊이) 로 표현할 수 있다. 여기서 분기란 재귀 함수가 자신을 재호출 하는 횟수를 뜻한다.","link":"/2020/01/23/%EC%8B%9C%EA%B0%84%EB%B3%B5%EC%9E%A1%EB%8F%84-big-O/"},{"title":"탑_2493번","text":"[Python] 백준 2493번 풀이탑 순서대로 주어지는 탑의 높이를 기준으로 전파가 닿는곳을 구하기 혹은 닿지 않는지를 판단하는 스텍 문제이다. 입력되는 탑의 높이를 스텍에 담아두고 판별한다. 스텍에 특정 높이의 탑이 있을 때, 다음에 들어가는 탑의 높이가 더 높다면 스텍에서 top을 지우면 된다. 왜냐하면, 스텍의 top보다 높은 높이의 탑을 입력받았다면 그 이후에 입력받는 모든 탑들의 전파는 스텍 top에 있는 탑에는 절대 도달할 수 없기 때문이다. 만약 스텍에 판단할 수 있는 top이 없다면 스텍에 입력을 집어넣고 0 을 출력하면 된다. 스텍에 특정 높이의 탑이 있을 때, 다음에 들어가는 탑의 높이가 더 낮다면 스텍에 입력받은 탑의 높이를 top으로 입력하고 이전 top의 위치를 출력해준다. 왜냐하면, 이후에 입력받은 탑의 높이까지 판단해야 하기 때문에 출력과 동시에 스텍의 top으로 저장해준다. 주의할점은, 이렇게 전파가 가로막혀 위치를 출력했다면 while 루프를 벗어나서 다음에 입력받는 탑의 높이로 돌아가야 한다는 것이다. 전파가 가로막혔을 때, 해당 탑의 위치를 출력해야 하는 문제이므로 스텍안에 “(인덱스, 탑의 높이)” 를 갖는 튜플을 저장했다. python 풀이 1234567891011121314151617181920212223242526import sysn = int(sys.stdin.readline())tower = list(map(int, sys.stdin.readline().split()))stack = []for idx,i in enumerate(tower): # print(stack) while stack: if i &gt; stack[-1][1]: stack.pop() elif i &lt; stack[-1][1]: print(stack[-1][0], end=\" \") stack.append((idx+1,i)) break if not stack: stack.append((idx+1,i)) print(0, end=\" \") else: continue Full CodeFull Code - Python","link":"/2020/01/25/%ED%83%91-2493%EB%B2%88/"},{"title":"CtCI_Ch1_배열과_문자열","text":"Ch1. 배열과 문자열코딩 인터뷰 완전 분석(저 : 게일 라크만 맥도웰) 참조 해시테이블 해시테이블(파이썬의 dict)은 키와 값으로 대응되는 자료구조이다. 해시코드 충돌로 인해 최악의 수행 시간은 O(N)이지만, 일반적으로 탐색 시간은 O(1)이다. python dict time complexity에서 시간 복잡도를 확인할 수 있다. 리스트 가변 크기의 자료구조를 원할 때는 리스트(배열)을 사용하게 된다. 리스트는 O(1)의 접근 시간을 유지한다. 리스트의 크기를 두배로 늘리는 시간은 O(n)이지만, 자주 발생하는 일이 아니라서 상환 입력 시간은 O(1)이 된다. Go 를 생각해보자. 슬라이스에 아이템을 하나씩 append 하다가 처음에 정의한 슬라이스의 크기보다 더 들어가야 된다면 그때, 슬라이스의 크기가 두배 늘어나는 상황이다. 상환 입력 시간이 O(1)인 이유 크기가 N인 배열을 생각해보자. N개의 원소를 삽입하기 위해 얼마나 많은 원소를 복사해야 하는지 역으로 계산할 수 있다. 12345678마지막 배열 크기 증가 : n/2 개의 원소 복사이전 배열 크기 증가 : n/4 개의 원소 복사이전 배열 크기 증가 : n/8 개의 원소 복사...두 번쨰 배열 크기 증가 : 2 개의 원소 복사첫 번째 배열 크기 증가 : 1 개의 원소 복사 즉, N개의 원소를 삽입하기 위해 복사해야 하는 원소의 총 개수는 1+2+…N/8+N/4+N/2 이며 이는 N보다 작다. 따라서, O(N)이 소요되는 삽입 연산도 존재하기는 하지만 평균적으로 삽입 연산은 O(1)이 소요된다. Ch1. 연습문제 코드CtCI_Ch1_Python","link":"/2020/01/30/CtCI-Ch1-%EB%B0%B0%EC%97%B4%EA%B3%BC-%EB%AC%B8%EC%9E%90%EC%97%B4/"},{"title":"CtCI_Ch3_스택과큐","text":"Ch3. 스택과 큐코딩 인터뷰 완전 분석(저 : 게일 라크만 맥도웰) 참조 스택 구현하기 스택은 데이터를 쌓아올리는 자료구조이다. 스택은 LIFO(Last-In-First-Out)에 따라 자료를 배열한다. 즉, 가장 최근에 스택에 추가한 항목이 가장 먼저 제거될 항목이 된다. 스택에는 다음과 같은 연산이 존재한다. pop(): 스택에서 가장 위에 있는 항목을 제거한다. push(item): item을 스택의 가장 위에 추가한다. peek(): 스택의 가장 위에 있는 항목을 반환한다. isEmpty(): 스택이 비어있을 때에 True를 반환한다. 파이썬을 통해 스택을 아래와 같이 구현할 수 있다. 12345678910111213141516171819202122232425262728class Stack(): def __init__(self): self.stack = [] def __len__(self): return len(self.stack) def isempty(self): if self.stack: return False else: return True def push(self, num): self.stack.append(num) def pop(self): if self.stack: return self.stack.pop() else: raise Exception(\"stack is empty\") def peek(self): if self.stack: return self.stack[-1] else: raise Exception(\"stack is empty\") 큐 구현하기 큐는 FIFO(First-In-First-Out)에 따라 자료를 배열한다. 즉, 큐에 저장되는 항목들은 추가되는 순서대로 제거된다. 큐에는 다음과 같은 연산이 존재한다. add(item): item을 리스트의 끝부분에 추가한다. remove(item): 리스트의 첫 번쨰 항목을 제거한다. peek(): 큐에서 가장 위에 있는 항목을 반환한다. isEmpty(): 큐가 비어있을 때 True를 반환한다. 파이썬을 통해 아래와 같이 큐를 구현할 수 있다. 123456789101112131415161718192021222324252627class Queue(): def __init__(self): self.queue = [] def push(self, num): self.queue.append(num) def pop(self): if self.queue: item = self.queue[0] self.queue = self.queue[1:] return item else: raise Exception(\"Queue is empty\") def peek(self): if self.queue: return self.queue[0] else: raise Exception(\"Queue is empty\") def isEmpty(self): if not self.queue: return True else: return False Ch3. 연습문제 코드CtCI_Ch3_Python","link":"/2020/02/04/CtCI-Ch3-%EC%8A%A4%ED%83%9D%EA%B3%BC%ED%81%90/"},{"title":"CtCI_Ch4_트리와_그래프","text":"Ch4-1. 트리코딩 인터뷰 완전 분석(저 : 게일 라크만 맥도웰) 참조 이진 트리 이진 트리는 각 노드가 최대 두 개의 자식 노드를 갖는 트리를 말한다. 자식 노드가 세개인 경우는 삼진 트리라고 부른다. 아래와 같은 트리는 최대 자식 노드가 두개이므로 이진 트리이다. 이진 탐색 트리 이진 탐색 트리는 모든 노드가 다음과 같은 순서를 따르는 이진 트리를 뜻한다. “모든 왼쪽 자식 노드 &lt;= n &lt; 모든 오른쪽 자식 노드” 조건이 모든 노드 n에 대해서 반드시 참인 경우 이진 탐색 트리이다. 조건은 바로 아래 자식 노드뿐 아니라 모든 자식 노드에 대해서 참이어야 한다. 위와 같은 경우, 모든 노드가 조건을 만족하므로 이진 탐색 트리이다. 예를 들어, 13은 8과 10보다는 크지만 14보다는 작으며 4는 8과 6보다는 작지만 3보다는 크다. 트리의 종류 완전 이진 트리(complete binary tree) 완전 이진 트리란 트리의 마지막 단계(Level)을 제외한 모든 높이에서 노드가 꽉 차 있어야 한다. 마지막 단계는 꽉 차 있지 않아도 되지만 노드가 왼쪽에서 오른쪽으로 채워져야 한다. 위의 그림은 완전 이진 트리가 맞지만 만약 L 노드가 F 노드의 right child 였다면 완전 이진 트리가 아니다. 전 이진 트리(full binary tree) 전 이진 트리는 모든 노드가 자식이 없거나 정확히 두 개 있는 경우이다. 다시 말해, 자식이 하나만 있는 노드가 존재해서는 안된다. 포화 이진 트리(perfect binary tree) 포화 이진 트리는 완전 이진 트리이면서 전 이진 트리인 경우이다. 모든 말단 노드가 같은 높이에 있고, 말단 노드의 개수가 최대이다. 포화 이진 트리의 노드 개수는 2k+1-1 개 이다. (k는 트리의 height) 이진 트리 순회 중위 순회(in-order traversal) 중위 순회는 왼쪽 가지, 현재 노드, 오른쪽 가지 순서로 노드를 방문하고 출력하는 방법이다. 이진 탐색 트리를 중위 순회한다면 오름차순으로 방문하게 된다. 전위 순회(pre-order traversal) 전위 순회는 자식 노드보다 현재 노드를 먼저 방문하는 방법이다. 전위 순회에서는 루트 노드를 가장 먼저 방문하게 된다. 후위 순회(post-order traversal) 후위 순회는 모든 자식 노드를 먼저 방문하고 마지막에 현재 노드를 방문하는 방법이다. 후위 순회에서는 루트 노드를 가장 마지막에 방문하게 된다. 이진 힙 힙은 최댓값 및 최솟값을 빠르게 찾아내기 위해 고안된 완전 이진 트리이다. 부모노드의 키값이 자식노드의 키값보다 항상 작은 힙을 “최소 힙” 이라 하며, 그 반대는 “최대 힙”이라 한다. 키값의 관계는 부모와 자식 노드 사이의 관계만 성립하며, 형제 사이에 대소관계는 성립하지 않는다. 삽입 연산 최소힙에 원소를 삽입할 때, 가장 밑바닥 오른쪽 위치로 삽입한다.(완전 트리의 속성에 위배되지 않기 위해서) 그 다음 삽입된 원소가 알맞은 자리를 찾을 때까지 부모 노드와 교환해 나간다. 이와 같은 방식으로 최소 원소를 위쪽으로 올린다. 이 연산은 힙에 있는 원소의 개수를 n개라 할 때, O(long n) 시간이 걸린다. 최소 원소 뽑아내기 최소힙에서 최소 원소는 언제나 가장 위에 위치한다. 최소 원소를 제거한 후, 힙에 있는 가장 마지막 원소와 교환한다. 그 다음 최소힙의 성질을 만족하도록 해당 노드를 자식 노드와 교환한다. 자식 노드와 교환할 때 왼쪽과 오른쪽 자식 중 누구와 교환해야 할까? 최소힙의 속성을 유지하기 위해 더 작은 원소와 교환해 나간다. 트라이(접두사 트리) 트라이는 n-차 트리의 변형으로 각 노드에 문자를 저장하는 자료구조이다. 트리를 아래쪽으로 순회하면 단어 하나가 나오게 된다. 널 노드(null node) 혹은 ‘ * 노드 ‘ 는 종종 단어의 끝을 나타내주기 위해 사용된다. 트라이에서 각 노드는 1개에서 &lt;알파벳_사이즈 + 1&gt; 개까지 자식을 갖고 있을 수 있다. 트라이를 통해 유효한 단어의 접두사를 빠르게 확인할 수 있으며 이는 길이가 K인 문자열이 주어졌을 때 O(K) 시간이 소요된다. 참고 https://algorithms.tutorialhorizon.com/binary-min-max-heap/ https://www.geeksforgeeks.org/","link":"/2020/02/12/CtCI-Ch4-%ED%8A%B8%EB%A6%AC%EC%99%80-%EA%B7%B8%EB%9E%98%ED%94%84/"},{"title":"CtCI_Ch4_트리와_그래프_2","text":"Ch4-1. 그래프코딩 인터뷰 완전 분석(저 : 게일 라크만 맥도웰) 참조 그래프 앞서 이야기한 트리는 그래프의 한 종류이다. 하지만 모든 그래프가 트리는 아니다. 트리는 사이클이 없는 하나의 연결 그래프라고 말할 수 있다. 그래프는 노드와 노드를 연결하는 간선을 하나로 모아 놓은 것이다. 그래프에는 방향성이 있을 수도 있고 없을 수도 있다. 방향성이 있는 간선은 일방통행, 방향성이 없는 간선은 양방향 통행이라고 생각하면 된다. 모든 노드 쌍에 대해 경로가 존재하는 그래프를 “연결 그래프”라고 한다. 그래프에는 사이클이 있을 수도 없을 수도 있다. 사이클이 없는 그래프를 “비순환 그래프(acyclic graph)”라고 한다. 그래프 탐색 깊이 우선 탐색(depth-first search) 깊이 우선 탐색(DFS)은 루트 노드에서 시작해서 다음 분기로 넘어가기 전 해당 분기를 완벽하게 탐색하는 방법이다. 즉, a 노드를 방문한 뒤 a 와 이웃한 노드 b를 방문했다면, a 와 인접한 다른 노드를 방문하기 이전에 b 의 이웃 노드들을 전부 방문해야 한다. 전위 순회를 포함한 트리 순회는 모두 DFS의 한 종류이다. DFS는 그래프에서 모든 노드를 방문하고자 할 때 더 선호되는 편이다. 너비 우선 탐색(breadth-first search) 너비 우선 탐색(BFS)은 a 노드에서 시작했을 때, a 노드의 이웃 노드를 모두 방문한 다음에 이웃의 이웃들을 방문한다. 즉, BFS는 a 에서 시작해서 거리에 따라 단계별로 탐색한다고 봉 수 있다. BFS는 재귀적으로 동작하지 않는다. BFS는 큐를 이용해서 방문할 노드를 저장하고 이를 탐색한다. 양방향 탐색(bidirectional search) 양방향 탐색은 출발지와 도착지 사이의 최단 경로를 찾을 때 사용된다. 출발지와 도착지 두 노드에서 동시에 너비 우선 탐색을 수행한 뒤, 두 탐색 지점이 충돌하는 경우에 경로를 찾는 방식이다. 노드 s에서 노드 t까지 너비 우선 탐색으로 4단계를 거쳐야하는 경우를 생각해보자. s와 t 두 지점에서 동시에 탐색을 시작하면 각 노드의 2단계 이후에 탐색 지점이 충돌한다. 이를 수행시간으로 비교해보자. 전통적인 너비 우선 탐색을 통해 k개의 노드를 탐색하는 과정을 d번 반복하면 O(kd) 개의 노드를 탐색해야 한다. 양방향 탐색을 이용한다면 두 노드의 중간 지점 정도인 d/2 반복에서 충돌할 것이다. 따라서 s와 t 각각에서 방문하게 될 노드의 수는 대략 kd/2 개가 될 것이고, 전체 노드는 2*kd/2, O(kd/2) 의 수행시간이 된다. kd/2 * kd/2 = kd 인 것을 생각해 보면, 양방향 탐색은 일반적인 너비 우선 탐색보다 kd/2 만큼 더 빠르다. 참고 https://www.codesdope.com/course/algorithms-bfs/ https://www.codesdope.com/course/algorithms-dfs/","link":"/2020/02/12/CtCI-Ch4-%ED%8A%B8%EB%A6%AC%EC%99%80-%EA%B7%B8%EB%9E%98%ED%94%84-2/"},{"title":"CtCI_Ch5_비트_조작","text":"Ch5. 비트 조작코딩 인터뷰 완전 분석(저 : 게일 라크만 맥도웰) 참조 산술 우측 시프트와 논리 우측 시프트 산술 우측 시프트(arithmetic right shift) 산술 우측 시프트는 기본적으로 2를 나눈 결과와 같다. 이는 비트를 오른쪽으로 옮기긴 하지만 부호비트는 바꾸지 않는다. 논리 우측 시프트(logical right shift) 논리 우측 시프트는 비트를 옆으로 옮긴 뒤 최상위 비트(most significant bit)에 0을 넣는다. 비트 조작하기 비트값 확인 i 번째 비트의 값이 0인지 1인지 확인하고자 한다. 이를 위해 1을 i만큼 시프트해서 00010000 과 같은 값을 만든다. 이를 AND 연산을 통해 i 번째 비트를 제외한 나머지 비트를 모두 삭제한 뒤, 이 값을 0 과 비교한다. 만약 이 값이 0이 아니라면 i 번째 비트는 1일 것이며, 0이라면 i 번쨰 비트는 0이어야 한다. 12def getBit(num, i): return ((num &amp; (1&lt;&lt;i)) != 0) 위의 함수를 통해 i 번째 비트의 값을 확인할 수 있다. 비트값 채워넣기 i 번째 비트를 1로 바꾸려고 한다. 이를 위해 1을 i만큼 시프트해서 00010000 과 같은 값을 만든다. 이를 OR 연산을 통해 num의 i번째 값만을 바꿔줄 수 있다. 12def setBit(num, i): return num | (1 &lt;&lt; i) 위의 함수를 통해 i 번쨰 비트를 1로 바꿔줄 수 있다. 비트값 삭제하기 i 번쨰 비트값만 삭제하고자 한다. 이를 위해 11101111 과 같은 마스크를 만들어야 한다. 1 을 num 의 길이보다 1 크게 쉬프트한 뒤 1을 빼서 11111111 과 같은 값을 만든다. 그리고 1을 i 만큼 쉬프트해서 00010000 과 같은 값을 만든다. 만든 두 개의 값을 XOR 연산을 통해 11101111 과 같은 마스크를 만든다. 마스크와 num 을 AND 연산하면 i 번쨰 비트만 삭제된다. 123456def clearBit(num, i): length = len(bin(num))-2 temp1 = (1&lt;&lt;(length+1)) -1 ## 11111111 만들기 temp2 = 1&lt;&lt;i ## 00010000 만들기 mask = temp1 ^ temp2 ## 두개를 합쳐서 마스크 만들기 return num &amp; mask 위의 함수를 통해 i 번쨰 비트만 삭제할 수 있다. Ch5. 연습문제 코드CtCI_Ch5_Python","link":"/2020/03/04/CtCI-Ch5-%EB%B9%84%ED%8A%B8-%EC%A1%B0%EC%9E%91/"},{"title":"파이썬_딕셔너리___missing___메서드에_관하여.","text":"파이썬 딕셔너리 missing 메서드에 관하여 파이썬 딕셔너리를 사용할 때 찾는 key값이 없는 경우 다음과 같은 에러가 발생한다. 12345mine2 = dict()mine2[\"dog\"] = [\"Na\", \"Mg\"]print(mine2[\"cat\"])### print &gt;&gt; KeyError: 'cat' 이러한 KeyError 를 딕셔너리를 상속받는 자기만의 딕셔너리 클래스를 만들어서 해결할 수 있는 방법이 있다. missing 메서드는 딕셔너리의 key 값을 찾고싶다는 입력을 보냈을 때 키 값이 존재하지 않을 경우 어떤 값을 return 할까를 작성할 수 있다. (정확히는 getitem 에서 key가 없을 경우 정의된 missing 을 호출한다.) 다음과 같은 나만의 딕셔너리를 만들어보자. 123class mydict(dict): def __missing__(self, key): return [] 딕셔너리를 상속받은 나의 mydict에 다음과 같은 데이터를 넣어보자. 123456789mine = mydict()mine[\"dog\"] = [\"H\"]mine[\"tiger\"] = [\"He, Li\"]mine[\"wolf\"] = [\"Be\", \"B\", \"C\"]print(mine)### print &gt;&gt; {'dog': ['H'], 'tiger': ['He, Li'], 'wolf': ['Be', 'B', 'C']} 위와 같은 mine 딕셔너리에서 처음과 같이 “cat” 을 찾으려고 한다면 어떻게 될까? 123print(mine[&quot;cat&quot;])### print &gt;&gt; [] KeyError 가 발생하지 않았다. 왜냐하면 나만의 딕셔너리에 정의한 missing 메서드를 통해 key값이 없다면 빈 리스트를 return하게 만들었기 때문이다! 이를 이용하면 다음과 같이 데이터를 다룰 수도 있다. 12345mine[&quot;cat&quot;] += [&quot;N&quot;, &quot;O&quot;, &quot;F&quot;, &quot;Ne&quot;]print(mine)### print &gt;&gt; {'dog': ['H'], 'tiger': ['He, Li'], 'wolf': ['Be', 'B', 'C'], 'cat': ['N', 'O', 'F', 'Ne']} 그냥 처음과 같이 데이터를 입력하면 되는것이 아닌가? 라고 생각할 수 있지만 큐나 스텍을 이용해 순차적으로 모든 데이터를 순회하는 경우에는 이와같은 방법이 유용하게 쓰일 수 있다.","link":"/2020/03/17/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%94%95%EC%85%94%EB%84%88%EB%A6%AC-missing-%EB%A9%94%EC%84%9C%EB%93%9C%EC%97%90-%EA%B4%80%ED%95%98%EC%97%AC/"},{"title":"파이썬_dataframe_function_apply_방법_without_iterrrows","text":"파이썬 dataframe 각 row에 function을 apply하는 방법에 대하여. dataframe을 다루다보면 각 행(row)에 대해 특정 값에 대한 결과를 얻거나 과정을 수행하고 싶을 때가 있다. 이럴때 가장먼저 떠오르는것은 pd.iterrows() 였다. 하지만 천만건이 넘어가는 데이터를 다루려 하다보니 iterrows는 너무나 느리다는 것을 알게 되었다. 이를 해결하기 위해 찾아낸 방법이 수행할 작업을 function으로 만들고 각 행에 apply하는 것이다. 간단한 dataframe 을 만들어서 살펴보자. 12345678910import pandas as pdrectangles = [ { 'Name': \"A\", 'Age': 17 }, { 'Name': \"B\", 'Age': 20 }, { 'Name': \"C\", 'Age': 27 }]rectangles_df = pd.DataFrame(rectangles)print(rectangles_df) Name Age A 17 B 20 C 27 이름과 성별이 주어진 dataframe이 있을 때 각 행을 살펴보며 어떤 사람이 투표권을 행사할 수 있을지 판단해보려고 한다. 각 행을 입력받아 Age 가 18세 이상이라면 True를 그렇지 않다면 False를 반환하는 함수를 만든뒤 datafrmae에 apply한다. 12345678def vote(row): if row['Age'] &gt;= 18: return True else: return Falserectangles_df[\"vote\"] = rectangles_df.apply(vote, axis=1)print(rectangles_df) Name Age vote A 17 False B 20 True C 27 True 위와 같이 각 행의 Age 를 판단해 vote라는 새로운 칼럼을 만들어낸 것을 볼 수 있다. 이와같이 함수를 dataframe에 apply할 때 각 행(row)에 대하여 적용하고 싶다면 df.apply(func, axis=1) 로 axis=1 을 잊지말아야 한다. 위의 예제는 행이 3개라 iterrows로 실습해도 차이를 느낄순 없을것이다. 하지만 데이터 크기가 커질수록 iterrows를 사용하는것은 자제하고 apply를 적용하는 방법을 알아야한다. apply 외에도 몇가지 다른 방법을 알고 싶다면 참고해보자.","link":"/2020/03/24/%ED%8C%8C%EC%9D%B4%EC%8D%AC-dataframe-function-apply-%EB%B0%A9%EB%B2%95-without-iterrrows/"},{"title":"파이썬_dataframe_loc_iloc_인덱싱","text":"파이썬 dataframe 인덱싱하기 dataframe을 인덱싱할때 loc과 iloc을 사용할 수 있다. 다음과 같은 데이터 프레임을 예시로 두 가지를 살펴보자. 1234567891011121314151617import pandas as pddf = pd.DataFrame({ 'age' : [13,17,19,21,23], 'class' : ['math','science','english','math','science'], 'city' : ['A','B','A','C','B'], 'gender' : [ 'M', 'F', 'F', 'M', 'M'],})print(df)&gt;&gt;&gt; age class city gender0 13 math A M1 17 science B F2 19 english A F3 21 math C M4 23 science B M loc을 사용하여 인덱싱하기 loc 인덱싱은 두 가지 사용법이 있다. 12345df.loc[행 인덱스값]ordf.loc[행 인덱스값, 열 인덱스값] 이를 이용해 둘째, 셋째 행만을 가져오면 다음과 같다. 1234567ex1 = print(df.loc[1:2])print(ex1)&gt;&gt;&gt; age class city gender1 17 science B F2 19 english A F boolean 시리즈를 행을 선택하는 인덱스값으로 사용할 수 있다. 이를 이용하여 A 도시에 사는 행을 선택하면 다음과 같다. 1234567ex2 = df.loc[df.city == 'A']print(ex2)&gt;&gt;&gt; age class city gender0 13 math A M2 19 english A F 한 걸음 더 나가서 A 도시에 사는 사람들의 성별을 나타내는 시리즈를 알고 싶다. 이럴때는 열 인덱스값을 추가해서 다음과 같이 얻어낼 수 있다. 123456ex3 = df.loc[df.city == 'A', 'gender']print(ex3)&gt;&gt;&gt;0 M2 F 이렇게 열 인덱스값을 추가하여 다음과 같이 나이와 성별만을 갖는 두개의 행으로 datframe을 슬라이싱 할 수도 있다. 1234567ex4 = df.loc[1:2,['age','gender']]print(ex4)&gt;&gt;&gt; age gender1 17 F2 19 F iloc을 사용하여 인덱싱하기 iloc은 integer-location based indexing 로서 정수(integer)를 인덱스값으로 받는다는 점이 loc과의 차이점이다. loc의 마지막 예제를 iloc을 통해 나타내면 다음과 같다. 1234567ex5 = df.iloc[1:3,[0,3]]print(ex5)&gt;&gt;&gt; age gender1 17 F2 19 F ex4와 ex5의 코드를 살펴보면 단순히 정수값을 받는다는 점 이외에도 차이점이 있다. loc 은 행 인덱스값을 1:2 까지로 나타내면 마지막 행까지 모두 포함한 결과를 내놓지만, iloc은 행 인덱스값의 마지막 행을 포함하지 않기 때문에 1:3 으로 나타내줬다.","link":"/2020/03/29/%ED%8C%8C%EC%9D%B4%EC%8D%AC-dataframe-loc-iloc-%EC%9D%B8%EB%8D%B1%EC%8B%B1/"},{"title":"Pytorch_Linear_regression_기본정리","text":"Pytorch Linear regression 기본 정리모두를 위한 딥러닝 - 파이토치 강의 참고 하나의 x값을 통해 하나의 y를 예측하는 간단한 모델을 만들어보자. 다음과 같이 세개의 데이터를 만들고 이를 통해 간단한 선형 예측 모델을 학습시켜본다. 12x_train = torch.FloatTensor([[1], [2], [3]])y_train = torch.FloatTensor([[2], [4], [6]]) 선형 모델에서 x값이 주어졌을 때 y값(hypothesis-H(x))을 계산하기 위해서는 다음과 같은 식을 따르게 된다. 1H(x) = W * x + b 가지고 있는 데이터를 위 식을 통해 학습시키고자 한다면 우리가 알아야 하는 값 즉, 학습해야 하는 값은 W 와 b이다. “requires_grad=True” 를 설정하여 다음과 같이 0으로 초기화된 W 와 b 를 만들 수 있다. 12w = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True) 우리가 만든 모델을 평가하기 위해서는 Mean Squared Error(MSE) 를 사용할 것입니다. MSE는 예측값과 실제값의 차이를 제곱한 후 평균을 구한 값으로 다음과 같이 나타낼 수 있습니다. 1loss = torch.mean((hypothesis - y_train) ** 2) loss 를 구했다면 이를 통해 학습해야 하는 W 와 b 값을 개선해야 합니다. 이를 위해 Stochastic gradient descent(SGD)를 사용할 것입니다. optim 라이브러리의 SGD에 학습할 tensor를 리스트 형태로 넣어주고 learing rate를 지정해 다음과 같이 나타낼 수 있습니다. 1optimizer = torch.optim.SGD([w, b], lr=0.01) optimizer를 정의했다면 아래와 같은 순서를 통해 W와 b를 개선해준다. 123optimizer.zero_grad() # gradient 초기화loss.backward() # gradient 계산optimizer.step() # W 와 b 개선 이러한 학습 방법을 따라 모델을 100회 한 후 x가 4일때의 y값을 살펴보면 8에 근접하는 값이 나오는 것을 확인할 수 있다. 123x_test = torch.FloatTensor([[4]])print(x_test * w + b)#&gt;&gt;&gt; tensor([[7.5598]], grad_fn=&lt;AddBackward0&gt;) Full CodeFull Code","link":"/2020/04/02/Pytorch-Linear-regression-%EA%B8%B0%EB%B3%B8%EC%A0%95%EB%A6%AC/"},{"title":"Pytorch_Multivariable_Linear_regression","text":"Pytorch Multivariable Linear regression 기본 정리모두를 위한 딥러닝 - 파이토치 강의 참고 두 개 이상의 x값으로부터 y값을 예측하는 간단한 모델을 만들어보자. 간단한 예제를 위해 (5,3) 의 train_x data를, (5,1) 의 train_y 데이터를 만든다. 123456x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 83], [89, 91, 90], [96,98, 100], [73, 66, 70]])y_train = torch.FloatTensor([[152], [185], [180], [196], [142]]) 선형 회귀 모델을 만들것이므로 우리가 학습해야하는 변수는 w 와 b 두 가지이다. 이를 torch.zeros 를 이용해 만들고 requires_grad=True 로 설정해 학습할 데이터로 설정하자. 12w = torch.zeros((3,1), requires_grad=True)b = torch.zeros(1, requires_grad=True) 모델을 통해 구한 hypothesis와 실제값의 차이로부터 loss를 구하기 위해 MSE를 사용하고, SGD optimizer를 통해 w 와 b를 개선한다. 1optimizer = torch.optim.SGD([w,b], lr=1e-5) epochs 수만큼 모델을 학습시킨다. 12345678910111213for epoch in range(epochs+1): hypothesis = x_train.matmul(w) + b # 모델을 통해 구해지는 predict값 loss = torch.mean((hypothesis - y_train) ** 2) # predict와 train 데이터로부터 구하는 loss optimizer.zero_grad # gradient 초기화 loss.backward() # gradient 계산 optimizer.step() # w 와 b 개선 print('Epoch {:4d}/{} hypothesis : {} Cost : {:.6f}'. format( epoch, epochs, hypothesis.squeeze().detach(), loss.item() )) Full CodeFull Code","link":"/2020/04/06/Pytorch-Multivariable-Linear-regression/"},{"title":"Pytorch_data_loading_with_DataLoader","text":"Pytorch DataLoader를 통한 data loading모두를 위한 딥러닝 - 파이토치 강의 참고 DataLoader를 통해 데이터를 batch_size 만큼 나누어 읽어오기 위해서는 torch.utils.data 의 Dataset을 상속받는 클래스를 정의해야 합니다. 자기만의 Dataset을 만든 뒤, __len__ 과 __getitem__ 메서드를 overriding해서 사용해야 합니다. 12345def __len__(self): ...def __getitem__(self, idx): ... Dataset의 소스는 이곳에서 확인할 수 있으며, DataLoader에 대한 한글 설명은 이곳을 참고할 수 있습니다. 간단한 Linear regression 모델과 데이터를 만들어서 실습해 보았습니다. 먼저 Dataset을 상속받는 저만의 Dataset과 __len__ 과 __getitem__ 메서드를 만들어 보겠습니다. 1234567891011121314151617class MyDataset(Dataset): def __init__(self): self.x_train = [[73, 80, 75], [93, 88, 83], [89, 91, 90], [96,98, 100], [73, 66, 70]] self.y_train = [[152], [185], [180], [196], [142]] def __len__(self): return len(self.x_train) def __getitem__(self, idx): x = torch.FloatTensor(self.x_train[idx]) y = torch.FloatTensor(self.y_train[idx]) return x, y 이렇게 만들어진 MyDataset은 torch.utils.data의 DataLoader를 통해 batch_size를 조절할 수 있습니다. 12dataset = MyDataset()dataloader = DataLoader(dataset, batch_size=2, shuffle=True) # shuffle 은 데이터를 섞어서 각각의 배치를 만들어준다는 뜻이다. DataLoader를 통해 만들어진 객체는 iterable한 객체이기 때문에 다음과 같이 출력해서 확인해 볼 수도 있습니다. 123456789101112a = iter(dataloader)print(next(a))print(next(a))print(next(a))#&gt;&gt;&gt;[tensor([[ 73., 80., 75.], [ 96., 98., 100.]]), tensor([[152.], [196.]])][tensor([[73., 66., 70.], [93., 88., 83.]]), tensor([[142.], [185.]])][tensor([[89., 91., 90.]]), tensor([[180.]])] 저만의 MyDataset에 입력한 데이터가 DataLoader를 통해 지정한 배치로 나눠져서 정상적으로 출력되는것을 확인할 수 있습니다. 이제 Linear regression 모델, optimizer를 다음과 같이 만든 뒤, 1234567891011class MultivariateLinearRegressionModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3, 1) def forward(self, x): return self.linear(x)model = MultivariateLinearRegressionModel()optimizer = optim.SGD(model.parameters(), lr=1e-5) DataLoader를 통해 데이터를 불러와 학습해 보겠습니다. 1234567891011121314151617epochs = 20for epoch in range(epochs + 1): for batch_idx, train in enumerate(dataloader): xt, yt = train prediction = model(xt) loss = F.mse_loss(prediction, yt) optimizer.zero_grad() loss.backward() optimizer.step() print('Epoch {:4d}/{} Batch : {}/{} Cost : {:.6f}'.format( epoch, epochs, batch_idx+1, len(dataloader), loss.item() )) epochs 수만큼 학습을 하는 for문 안에 또 하나의 for문이 들어가 있는것을 확인할 수 있습니다. 안쪽 for문에서 batch_size만큼 나눠진 train 데이터를 model을 통해 학습하는 과정을 구현해 볼 수 있었습니다. Full CodeFull Code","link":"/2020/04/07/Pytorch-data-loading-with-DataLoader/"},{"title":"Pytorch_Logistic_regression","text":"Pytorch Logistic regression(Binary Classifier만들기)모두를 위한 딥러닝 - 파이토치 강의 참고 0과 1 두 가지를 분류하기 위한 binary classifier를 만들어 보았습니다. binary 분류 문제를 해결하기 위해서는 선형 회구와 같은 실수값이 아닌 확률값을 예측해야 합니다. 이를 위해 선형 함수와 sigmoid 함수를 통과하는 BinaryClassifier를 다음과 같이 만듭니다. 1234567891011# 모델 정의class BinaryClassifier(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(8, 1) self.sigmoid = nn.Sigmoid() def forward(self, x): return self.sigmoid(self.linear(x)) model = BinaryClassifier() 학습에 사용할 모델을 만들었으니 사용할 데이터를 불러오고 optimizer를 정의합니다. 1234567xy = np.loadtxt('data/data-03-diabetes.csv', delimiter=',', dtype=np.float32)x_data = xy[:, 0:-1]y_data = xy[:, [-1]]x_train = torch.FloatTensor(x_data)y_train = torch.FloatTensor(y_data)optimizer = optim.SGD(model.parameters(), lr=1) 마지막으로 epoch수 만큼 반복해 학습합니다. 1234567891011121314151617181920epochs = 100for epoch in range(epochs + 1): hypothesis = model(x_train) loss = F.binary_cross_entropy(hypothesis, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 10 == 0: prediction = (hypothesis &gt;= torch.FloatTensor([0.5])).float() correct_prediction = (prediction == y_train).float() accuracy = correct_prediction.sum().item() / len(correct_prediction) print('Epoch : {:4d}/{} loss : {:6f} Accuracy : {:2.2f}'.format( epoch, epochs, loss.item(), accuracy*100 )) Accuracy를 나타내줄때 올바른 예측은 hypothesis(predict) 값이 0.5보다 큰값을 기준으로 사용했습니다. binary 문제를 해결하기 위해서는 sigmoid함수를 사용했습니다. 다음 강의때 다시한번 적겠지만 세개 이상의 class를 가지는 분류문제를 해결하기 위해서는 softmax함수를 사용해야 한다는 차이점을 기억해야겠습니다. Full CodeFull Code","link":"/2020/04/08/Pytorch-Logistic-regression/"},{"title":"Pytorch_Multi_Layer_Perceptron_MLP","text":"Pytorch Multi Layer Perceptron(MLP)에 관하여 with code모두를 위한 딥러닝 - 파이토치 강의 참고 perceptron은 입력값 x에 대해 weight를 곱하고 bias를 더한 후 activation function을 거쳐서 나온 output을 통해 두가지의 class를 가지는 AND, OR 문제를 해결하기 위해 고안되었습니다. AND gate는 다음과 같이 표로 나타낼 수 있습니다. A B Output 0 0 0 0 1 0 1 0 0 1 1 1 위의 표는 매우 익숙하지만 이를 그래프로 나타내면 다음과 같이도 나타낼 수 있습니다. 위의 그래프에서 0과 1을(off와 on)을 선 하나를 통해서 구분할 수 있는 방법은 빨간 점선으로 표시되어 있습니다. 즉, 하나의 선으로 두 가지의 class를 구분할 수 있습니다. OR gate도 마찬가지 입니다. A B Output 0 0 0 0 1 1 1 0 1 1 1 1 위와 같은 OR gate표를 그래프로 나타내면 다음과 같습니다. OR gate 그래프에서도 0과 1(off와 on)을 하나의 선으로 구분할 수 있습니다. 즉, AND gate와 마찬가지로 하나의 선으로 두 가지의 class를 구분할 수 있다는 뜻입니다. 다시 말해, AND 와 OR gate는 하나의 Layer를 가지는 perceptron으로 구분이 가능했습니다. 하지만, XOR gate는 달랐습니다. XOR gate를 표로 나타내면 다음과 같습니다. A B Output 0 0 0 0 1 1 1 0 1 1 1 0 이와같이 입력이 같으면 0을 다르면 1을 되돌려주는 XOR gate를 그래프로 나타내면 다음과 같습니다. 위와 같은 그래프에서 0과 1(off와 on)을 하나의 선으로 구분할 수 없었습니다. 즉, 하나의 perceptron으로는 AND/OR gate 문제는 해결할 수 있었지만 XOR gate 문제를 해결할 수 없었습니다. 이렇게 하나의 perceptron으로 해결할 수 없는 문제를 해결하기 위해 등장한것이 Multi Layer Perceptron입니다. 다시 한번 위의 그래프를 봤을때, 하나의 선이 아니라 두개의 선으로는 0과 1(off와 on)을 구분할 수 있을까요? 답은 YES입니다. 즉, 두개 이상의 perceptron으로는 XOR gate문제를 해결할 수 있었습니다. 4개의 Layer를 갖는 모델을 구현해 확인해 보겠습니다. 먼저 XOR gate에 해당하는 데이터를 만들고, 123# XOR 데이터X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device) 4개의 layer를 생성해 모델을 정의해줍니다. 12345678910# 4개의 레이어가 있는 MLP(Multi Layer Perceptron)linear1 = torch.nn.Linear(2, 10, bias=True)linear2 = torch.nn.Linear(10, 10, bias=True)linear3 = torch.nn.Linear(10, 10, bias=True)linear4 = torch.nn.Linear(10, 1, bias=True)sigmoid = torch.nn.Sigmoid()# 모델 정의model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device) 이후 loss와 optimizer를 정의해주고, 123# loss / optimizer 정의criterion = torch.nn.BCELoss().to(device)optimizer = torch.optim.SGD(model.parameters(), lr=1) 모델을 통해 weight와 bias를 학습합니다. 123456789101112131415161718192021# 학습for epoch in range(10001): optimizer.zero_grad() hypothesis = model(X) loss = criterion(hypothesis, Y) loss.backward() optimizer.step() if epoch % 100 == 0: print(epoch, loss.item())#&gt;&gt;&gt; print0 0.6948983669281006100 0.6931558847427368200 0.6931535005569458...9800 0.000164150187629275029900 0.0001602156116859987410000 0.0001565046259202063 학습이 끝나면 실제값과 예측값이 잘 맞아 떨어지는지 확인합니다. 12345678910111213141516171819# 학습 후 실제값과 예측값 비교해보기with torch.no_grad(): hypothesis = model(X) predict = (hypothesis &gt; 0.5).float() accuracy = (predict == Y).float().mean() print('Hyptthesis : ', hypothesis.detach().cpu().numpy(), '\\nCorrect : ',predict.detach().cpu().numpy(), '\\nAccuracy : ',accuracy.item())#&gt;&gt;&gt; printHyptthesis : [[1.1168354e-04] [9.9982882e-01] [9.9984241e-01] [1.8533420e-04]] Correct : [[0.] [1.] [1.] [0.]] Accuracy : 1.0 위의 결과를 통해 Multi Layer Perceptron을 사용하면 XOR gate문제를 해결할 수 있음을 확인했습니다. 참고 Full CodeFull Code","link":"/2020/04/10/Pytorch-Multi-Layer-Perceptron-MLP/"},{"title":"Pytorch_Softmax_classification","text":"Pytorch Softmax Classificaton 을 통한 N개의 이벤트 분류하기모두를 위한 딥러닝 - 파이토치 강의 참고 binary classificaton 문제를 해결할 때 sigmoid함수를 사용했습니다. 정확히는 F.binary_cross_entropy 안에 sigmoid함수가 같이 녹아있는 형태로 loss함수를 사용했습니다. 하지만, 세개 이상의 분류문제를 해결하기 위해서는 sigmoid함수가 아닌 softmax함수를 사용해야 합니다. softmax함수는 아래와 같은 식으로 나타낼 수 있습니다. K개의 입력값을 0~1 사이의 값이 되도록 K개의 SUM으로 나눠주게 됩니다. 따라서 모든 확률의 합은 1이됩니다. 또한 입력값의 순서가 출력값의 순서와 같음을 확인할 수 있습니다. 이제 Pytorch를 통해 SoftmaxClassifierModel을 정의해 보았습니다. 12345678910# 모델 정의class SoftmaxClassifierModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(16, 7) def forward(self, x): return self.linear(x)model = SoftmaxClassifierModel() 모델을 정의한 내용을 보면 입력값X에 따른 Linear함수만을 통과하고 Softmax함수를 통과하지 않는것을 볼 수 있습니다. 이는 loss함수에서 사용하게 될 F.cross_entropy에 Softmax함수가 포함되어 있기 때문입니다. 따라서 정의한 모델에서는 Softmax를 통과하기전의 Linear함수를 통과한 output값을 되돌려줘야 합니다. optimizer를 정의하고, 12# optimizer 정의optimizer = optim.SGD(model.parameters(), lr=0.1) 정한 epoch수 만큼 학습하며 loss를 출력해봅니다. 123456789101112131415161718192021222324epochs = 1000# 학습for epoch in range(epochs + 1): predict = model(x_train) loss = F.cross_entropy(predict, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 100 == 0: print('Epoch {:4d}/{} loss : {:.6f}'.format( epoch, epochs, loss.item() )) ###&gt;&gt;&gt;printEpoch 0/1000 loss : 1.721660Epoch 100/1000 loss : 0.462462...Epoch 900/1000 loss : 0.110220Epoch 1000/1000 loss : 0.100880 출력된 loss를 통해 모델이 정상적으로 학습되는것을 확인할 수 있습니다. 이처럼 N개의 이벤트를 분류할 경우는 Softmax함수를 두개의 이벤트를 분류할 경우는 Sigmoid함수를 사용한다는것을 배울 수 있었습니다. Full CodeFull Code","link":"/2020/04/13/Pytorch-Softmax-classification/"},{"title":"Pytorch_ReLU","text":"Pytorch ReLU에 대하여모두를 위한 딥러닝 - 파이토치 강의 참고 이전 포스팅에서 두 가지 클래스를 분류하는 모델을 만들때 sigmoid함수를 사용했습니다. 하지만, sigmoid함수에는 모델이 깊어질수록 vanishing Gradient문제가 발생하게 됩니다. vanishing Gradient란, 역전파를 통해 gradient를 전파받을 때, 0에 근접한 값들이 곱해짐에 따라 앞단으로 갈수록 gradient가 사라지게 되는 문제입니다. 이러한 문제가 생기는 이유는 sigmoid 함수 그래프를 통해 확인할 수 있습니다. sigmoid 그래프의 양 극단으로 갈수록 기울기는 0에 수렴하게 되는것을 볼 수 있고, 이때문에 역전파를 통해 gradient가 곱해질 때마다 그 값 또한 0으로 수렴하게 됩니다. 따라서 아래 그림과 같이 역전파가 깊이 전달될수록 gradient가 사라지는 vanishing gradient 문제가 발생하게 됩니다. 이러한 문제를 해결하기 위해 Hinton교수님이 찾아낸 방법이 바로 ReLU 입니다. ReLU를 수식으로 나타내면 다음과 같습니다. 1f(x) = max(0, x) 입력값 x가 들어왔을때, 0보다 크다면 자기자신을 그렇지 않다면 0을 되돌려주는것이 바로 ReLU입니다. 코드에서 활성화 함수만 sigmoid에서 ReLU로 바꾸면 되기때문에 ReLU를 이용한 MNIST classifier 를 만들어보겠습니다. 12345# nn Linear layer 만들기linear1 = torch.nn.Linear(784, 256, bias=True)linear2 = torch.nn.Linear(256, 256, bias=True)linear3 = torch.nn.Linear(256, 10, bias=True)relu = torch.nn.ReLU() Linear layer와 활성화 함수로 사용할 ReLU를 정의해주고, Linear layer의 weight를 normalization 합니다. 1234# Linear layer의 weight를 normalization시켜주기torch.nn.init.normal_(linear1.weight)torch.nn.init.normal_(linear2.weight)torch.nn.init.normal_(linear3.weight) 위를 이용해 모델을 정의하고, 1234# 모델 정의model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)###&gt;&gt;&gt; 왜 linear3다음에는 relu를 하지않는가###&gt;&gt;&gt; 우리가 사용할 criterion은 CrossEntropyLoss인데 여기에는 마지막에 softmax activation이 포함되어 있기 때문이다. loss / optimizer 를 정의합니다. 123# loss / optimizer 정의criterion = torch.nn.CrossEntropyLoss().to(device)optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 정한 epoch수만큼 모델을 학습시킵니다. 1234567891011121314151617total_batch = len(data_loader)for epoch in range(epochs): avg_loss = 0 for X, Y in data_loader: X = X.view(-1, 28*28).to(device) Y = Y.to(device) hypothesis = model(X) loss = criterion(hypothesis, Y) optimizer.zero_grad() loss.backward() optimizer.step() avg_loss += loss / total_batch print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_loss)) 학습이 끝났다면 테스트 데이터에서 하나를 선택해 예측값과 실제값을 비교해 결과를 확인해봅니다. 12345678910111213141516with torch.no_grad(): # --&gt; 여기에서는(test에서는) gradient를 계산하지 않고 진행한다는 뜻이다. X_test = mnist_test.test_data.view(-1, 28*28).float().to(device) Y_test = mnist_test.test_labels.to(device) prediction = model(X_test) correct_prediction = torch.argmax(prediction, dim=1) == Y_test accuracy = correct_prediction.float().mean() print('Accuracy:', accuracy.item()) r = random.randint(0, len(mnist_test) - 1) X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device) Y_single_data = mnist_test.test_labels[r:r + 1].to(device) print('Label: ', Y_single_data.item()) single_prediction = model(X_single_data) print('Prediction: ', torch.argmax(single_prediction, 1).item()) 하나의 데이터를 통해 예측값과 실제값까지 확인해볼 수 있었습니다. Full CodeFull Code","link":"/2020/04/14/Pytorch-ReLU/"},{"title":"Pytorch_weight_initialization","text":"Pytorch Weight initialization에 대하여모두를 위한 딥러닝 - 파이토치 강의 참고 모델을 학습하기 이전에 weight의 초기값을 설정하는 문제는 모델의 학습에 굉장히 중요한 문제였습니다. 초기에 이 문제를 해결하기 위해서 사용했던 방법은 RBM(restricted boltzmann machine)이었습니다. RBM은 layer1과 다음 레이어 layer2를 통해 weight1을 학습한 후 w1을 고정한 상태에서 layer2와 layer3를 통해 weight2를 학습하는 과정을 반복하는 방법입니다. 하지만, 요즘에는 새로운 initialization 방법들이 제안되면서 RBM을 많이 사용하지 않고 있습니다. 대표적으로 간단히 initialization할 수 있는 두 가지 방법인 Xavier / He initialization 에 대해 알아보겠습니다. Xavier initialization Xavier / He initialization 모두 Normal initialization 방법과 Uniform initialization 방법을 가지고 있으며 모두 간단한 수식을 통해 이루어집니다. Xavier Normal initialization W\\sim N({ 0 }, Var(W)) Var(W)=\\sqrt{\\frac { 2 }{ { n }_{ in }+{ n }_{ out } } } nin은 layer의 input수, nout은 layer의 output수를 뜻합니다. Xavier Uniform initialization W\\sim U(- \\sqrt{\\frac { 6 }{ { n }_{ in }+{ n }_{ out } } } , \\space\\space + \\sqrt{\\frac { 6 }{ { n }_{ in }+{ n }_{ out } } } ) nin은 layer의 input수, nout은 layer의 output수를 뜻합니다. He initialization He Normal initialization W\\sim N({ 0 }, Var(W)) Var(W)=\\sqrt{\\frac { 2 }{ { n }_{ in } } } nin은 layer의 input수를 뜻합니다. He Uniform initialization W\\sim U(- \\sqrt{\\frac { 6 }{ { n }_{ in } } } , \\space\\space + \\sqrt{\\frac { 6 }{ { n }_{ in } } } ) nin은 layer의 input수를 뜻합니다. Pytorch에서는 torch.nn.init패키지를 통해서 사용할 수 있습니다. ReLU에 관해서 작성한 포스트에서 작성한 코드와 달라진 점은 weight를 초기화해주는 부분뿐입니다. 먼저 linear layer를 만들고, 1234linear1 = torch.nn.Linear(784, 256, bias=True)linear2 = torch.nn.Linear(256, 256, bias=True)linear3 = torch.nn.Linear(256, 10, bias=True)relu = torch.nn.ReLU() 다음과 같이 xavier_uniform으로 초기화 할 수 있습니다. 123torch.nn.init.xavier_uniform_(linear1.weight)torch.nn.init.xavier_uniform_(linear2.weight)torch.nn.init.xavier_uniform_(linear3.weight) ReLU 포스트의 코드와 다른점은 weight initialization뿐이지만 Accuracy가 상승된것을 확인할 수 있습니다. Full CodeFull Code","link":"/2020/04/16/Pytorch-weight-initialization/"},{"title":"Pytorch_Dropout","text":"Pytorch Dropout에 대하여모두를 위한 딥러닝 - 파이토치 강의 참고 Dropout은 모델의 layer가 많아질때 생기는 overfitting문제를 해결하기 위해 사용된다. overfitting이란 train데이터를 모델이 지나치게 정확히 학습하는 바람에 모델이 test데이터에서는 좋은 결과를 내놓지 못하는 경우이다. 위 그림에서 초록색선을 보면 빨간점과 파란점을 완벽하게 분류하는 것을 확인할 수 있다. 이렇게 train 데이터에 지나치게 학습된 경우를 overfitting이라하며, 적절한 학습의 정도를 까만선이 나타내고 있다. 이러한 overfitting문제를 해결하기 위해 dropout을 사용하게 된다. dropout이란 forward함수를 통해 train데이터가 layer를 지날때 뉴런 일부를 생략하고 학습을 진행하는것이다. 위의 우측 그림과 같이 layer마다 임의의 뉴런을 생략하고 학습을 진행하는 것을 볼 수 있다. 사용자가 정의한 비율만큼의 뉴런을 생략하고 학습을 진행할 때, 매 epoch마다 다른 뉴런들이 꺼졌다가 켜지기를 반복하게 된다. 이는 매번 다른 모델을 학습하는것과 유사하기 때문에 Ensemble(앙상블)효과를 기대할 수도 있다. dropout을 사용할 때는 non-linear 활성화 함수 다음에 사용하게 된다. MNIST 데이터셋을 이용해 활용법을 살펴보자. 통과할 linear 와 relu 그리고 dropout을 설정해준다. 12345678# nn Linear layer / relu / dropout 만들기linear1 = torch.nn.Linear(784, 512, bias=True)linear2 = torch.nn.Linear(512, 512, bias=True)linear3 = torch.nn.Linear(512, 512, bias=True)linear4 = torch.nn.Linear(512, 512, bias=True)linear5 = torch.nn.Linear(512, 10, bias=True)relu = torch.nn.ReLU()dropout = torch.nn.Dropout(p=drop_prob) # drop_prob = 0.3 in my code 이를 이용해 모델을 만들때 선형함수를 통과하고 활성화 함수를 지난 뒤에 dropout을 적용해준다. 123model = torch.nn.Sequential(linear1, relu, dropout, linear2, relu, dropout, linear3, relu, dropout, linear4, relu, dropout, linear5).to(device) 이렇게 만들어진 모델로 학습과 평가를 똑같이 진행할 수 없다. 왜냐하면 학습 이후 평가할 때 dropout이 켜져있다면 모든 weight를 사용하지 않고 output을 내게 되기 때문이다. 이렇게 dropout은 학습할때는 사용하고, 평가를 위해서는 사용하지 말아야한다. 이를 조절할 수 있는 함수가 train()함수와 eval()함수이다. 다음고 같이 train()함수를 통해 dropout을 사용하겠다는 표시를 한 후, 학습을 진행해야 한다. 1234567891011121314151617181920model.train() # train() 함수를 사용하면 dropout=True로 설정된다.# 즉 학습할때 사용해야하고 추후 모델을 평가할때는 eval()함수를 꼭 설정해줘야한다.for epoch in range(epochs): avg_loss = 0 for X, Y in data_loader: X = X.view(-1, 28*28).to(device) Y = Y.to(device) hypothesis = model(X) loss = criterion(hypothesis, Y) optimizer.zero_grad() loss.backward() optimizer.step() avg_loss += loss / total_batch print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_loss))print('Learning finished') 학습을 마친 후, 랜덤한 하나의 데이터를 통해 결과를 확인해 보고 싶다면 다음과 같이 eval()함수를 통해 dropout을 사용하지 않겠다고 표시한 후 평가를 진행해야 한다. 1234567891011121314151617with torch.no_grad(): # --&gt; 여기에서는(test에서는) gradient를 계산하지 않고 진행한다는 뜻이다. model.eval() # --&gt; eval() 함수를 사용하면 dropout=False 로 설정되서 모든 노드를 사용해 모델을 평가하게된다. X_test = mnist_test.test_data.view(-1, 28*28).float().to(device) Y_test = mnist_test.test_labels.to(device) prediction = model(X_test) correct_prediction = torch.argmax(prediction, dim=1) == Y_test accuracy = correct_prediction.float().mean() print('Accuracy:', accuracy.item()) r = random.randint(0, len(mnist_test) - 1) X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device) Y_single_data = mnist_test.test_labels[r:r + 1].to(device) print('Label: ', Y_single_data.item()) single_prediction = model(X_single_data) print('Prediction: ', torch.argmax(single_prediction, 1).item()) 첫째로, dropout은 non-linear 활성화 함수 다음에 사용한다는 점 둘째로, 학습과 평가를 위해서는 train()함수와 eval()함수를 반드시 사용해야 된다는 것을 기억해야겠다. Full CodeFull Code","link":"/2020/04/18/Pytorch-Dropout/"},{"title":"Pytorch_Batch_Normalization","text":"Pytorch Batch Normalization에 대하여모두를 위한 딥러닝 - 파이토치 강의 참고 overfitting과 gradient vanishing문제를 해결하기 위해 앞서서 ReLU 포스트와 Dropout 포스트를 적었었다. 이번 포스트에서는 또 다른 방법인 Batch Normalization방법을 정리하고자 한다. Batch Normalization은 모델이 깊어질수록 나타나는 Internal Covariate Shift를 해결하면서 등장했다. Internal Covariate Shift란 모델이 깊어질수록 output의 분포가 편향되는 문제를 말합니다. 이를 해결하기 위해 Batch마다 output을 Normalization하는 방식이 등장합니다. Batch Normalization도 dropout과 똑같이 train()과 eval()을 구분지어서 사용해야 합니다. 왜냐하면, Batch Normalization을 통해 평균, 분산과 학습하는 데이터 scale값 감마, shift값 베타를 저장해서 사용하기 때문입니다. 따라서, 학습을 위해서는 dropout때와 마찬가지로 train()함수를, 평가를 위해서는 저장된 값을 이용하기 위해 eval()함수를 먼저 작성해야합니다. Batch Normalization은 dropout과 다르게 활성화 함수 이전에 적용시킵니다. 이를 이용해 Batch Normalization을 사용한 경우와, 사용하지 않은 경우를 비교해 보겠습니다. Dropout 포스트때와 유사하지만 Batch Normalization을 다음과 같이 만들어서 두 가지 모델을 정의했습니다. 12345678910111213141516171819linear1 = torch.nn.Linear(784, 32, bias=True)linear2 = torch.nn.Linear(32, 32, bias=True)linear3 = torch.nn.Linear(32, 10, bias=True)relu = torch.nn.ReLU()bn1 = torch.nn.BatchNorm1d(32)bn2 = torch.nn.BatchNorm1d(32)# BatchNorm을 사용하지 않는 모델을 만들기 위한 Linear layer 만들기nn_linear1 = torch.nn.Linear(784, 32, bias=True)nn_linear2 = torch.nn.Linear(32, 32, bias=True)nn_linear3 = torch.nn.Linear(32, 10, bias=True)# 모델 정의bn_model = torch.nn.Sequential(linear1, bn1, relu, linear2, bn2, relu, linear3).to(device)nn_model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device) 이후, 학습을 진행할 때 각 epoch마다 평가를 진행하면서 Loss와 Accuracy를 저장시켜 line plot을 그려 비교해 보겠습니다. 12345678910111213141516171819202122232425262728# 학습 &amp; 각epoch 마다 평가for epoch in range(epochs): bn_model.train() for X, Y in train_loader: X = X.view(-1, 28*28).to(device) Y = Y.to(device) . . . with torch.no_grad(): bn_model.eval() # train셋을 통한 평가 bn_loss, nn_loss, bn_Acc, nn_Acc = 0, 0, 0, 0 for i, (X, Y) in enumerate(train_loader): X = X.view(-1, 28 * 28).to(device) Y = Y.to(device) . . . # test셋을 통한 평가 bn_loss, nn_loss, bn_Acc, nn_Acc = 0, 0, 0, 0 for i, (X, Y) in enumerate(test_loader): X = X.view(-1, 28 * 28).to(device) Y = Y.to(device) . . . 이후 train과 validation의 Loss와 Accuracy를 Line Plot으로 그려 확인할 수 있었습니다. 그림에서도 확인할 수 있듯이 Batch Normalization을 적용했을 때, validation의 Loss가 더 작은것을 확인할 수 있었습니다. 데이터 로딩 ~ 이미지 생성까지 생략된 부분은 아래 Full Code를 확인해주세요. Full CodeFull Code","link":"/2020/04/21/Pytorch-Batch-Normalization/"},{"title":"파이썬_visdom_port_변경하기","text":"파이썬 visdom의 port 변경하기 visdom의 기본포트는 8097포트이다. CNN모델 학습 Loss그래프를 그리기 위해 visdom을 사용하려 했으나 다음과 같은 에러가 발생했다. 1액세스 권한에 의해 숨겨진 소켓에 액세스를 시도했습니다 이는 이미 8097포트를 다른 프로세스에서 사용하고 있기 때문에 발생한다. 따라서, 해당 프로세스를 찾아야하는데 cmd에 다음과 같은 명령어로 사용중인 포트와 프로세스를 확인할 수 있다. 1netstat -ano 해당 프로세스를 찾았으면 좌측 끝의 PID를 확인하고 작업 관리자를 열어주자. 작업 관리자 –&gt; 서비스 탭에 들어가면 PID로 프로세스를 정렬할 수 있다. 그곳에서 포트를 사용중인 프로세르를 선택하고 서비스 센터로 접근해 작업을 종료하면 된다. 이후 다시, 1python -m visdom.server 명령어로 visdom 로컬 서버를 키면된다. 하지만, 8097 포트를 사용하는 프로세스를 찾을 수가 없었다. 방화벽 문제인지 공유기 문제인지 확실하지는 못했지만 원인을 찾을수가 없었기에 visdom의 포트를 변경해서 사용하기로 결정했다. visdom의 포트는 다음과 같이 변경할 수 있다. 1python -m visdom.server -port [포트번호] 예를들어, 포트번호를 9000으로 옮긴다면 다음과 같이 바꿀 수 있다. 1python -m visdom.server -port 9000 이제 코드에서 visdom 객체를 불러올 때 다음과 같이 정한 포트 번호를 사용해서 불러오기만 하면 된다. 1vis = visdom.Visdom(port=9000) 아래 예제를 실행해서 올바르게 출력되는것을 확인할 수 있다. 1vis.text('hello world', env=\"main\")","link":"/2020/04/25/%ED%8C%8C%EC%9D%B4%EC%8D%AC-visdom-port-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0/"},{"title":"Pytorch_Convolution_layer","text":"Pytorch Convolution layer에 대하여모두를 위한 딥러닝 - 파이토치 강의 참고 Convolution layer란 이미지 위에서 filter가 stride값만큼 움직이며 이미지와 filter가 겹쳐지는 부분의 각 원소의 값을 곱하고 모두 더한 값을 출력으로 하는 연산이다. 아래와 같은 이미지 위에서 stride가 1인 filter가 움직이며 나타나는 출력을 계산해보자. 12345678910111213위의 그림에서 아래와 같은 필터를 사용했을 때 나오는 결과값을 하나 계산해보자filter = [[1, 0, 1], [0,1, 0], [1, 0, 1]]3x3필터가 이미지와 처음 겹쳐졌을 때 다음과 같은 연산이 수행된다.output = (1*1) + (0*1) + (1*1) + (0*0) + (1*1) + (0*1) + (1*0) + (0*0) + (1*1) = 4따라서, 우측 Convolution Feature의 처음 값이 4로 나타나는 것을 확인할 수 있고, 이후에도 stride가 1이므로, 한칸씩 움직이며 같은 연산을 반복한다. zero-padding zero-padding이란 이미지 주변에 패드를 끼우듯 0으로 채워진 테두리를 둘러 inpute size를 키워주는 것이다. zero-padding을 사용하는 이유는 Convolution layer를 지났을 때 이미지의 크기를 보존해주기 위해서다. 위에서 우리는 3x3 의 filter를 사용한 결과, 원래 이미지의 크기가 5x5에서 3x3으로 줄어든 것을 볼 수 있었다. 이와같은 Convolution layer를 반복적으로 지날때 이미지의 크기가 작아져서 정보의 손실이 생기게된다. 하지만, zero-padding을 이용해 원본 이미지의 size를 키워서 Convolution layer를 지나게 한다면, 원본 이미지의 크기를 보존시켜 정보의 손실을 최소화할 수 있게된다. 위의 그림에서 Input size가 Output에서도 보존되는것을 확인할 수 있다. Output size 계산하기 Convolution layer를 지나쳤을때 나오는 output size는 input size, filter size, stride, padding에 의해 결정되며, 다음과 같은 공식을 따른다. 1output size = ((input size - filter size + (2*padding)) / strid) + 1 위의 이미지를 예시로 계산해보면 다음과 같다. 1output size = ((4x4 - 3x3 + (2*1)) / 1) + 1 = (1x1 + 2) + 1 = (3x3)+1 = 4x4 input size의 높이와 넓이가 다를때도 동일하게 계산할 수 있다. 이처럼 하나의 Convolution layer를 통과했을 때 나오는 output size를 계산할 수 있고 이를 다음 layer의 입력값으로 활용할 수 있게된다. 참고 Convolution layer 이미지 padding 이미지","link":"/2020/04/30/Pytorch-Convolution-layer/"},{"title":"Pytorch_MNIST_CNN","text":"Pytorch MNIST에 CNN모델 만들기모두를 위한 딥러닝 - 파이토치 강의 참고 Convolution layer를 활용해서 MNIST이미지를 학습시킬 CNN 모델을 만들어보자. 아래 CNN 구조와 비슷한 모델을 설계해본다. torch.nn.Sequential을 통해 위의 그림에서 나타내는 각 layer와 Fully Connected layer를 표현할 수 있다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344class CNN(torch.nn.Module): def __init__(self): super(CNN, self).__init__() self.drop_prob = 0.5 self.layer1 = torch.nn.Sequential( torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(2) ) self.layer2 = torch.nn.Sequential( torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(2) ) self.layer3 = torch.nn.Sequential( torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(2) ) self.fc1 = torch.nn.Linear(3*3*128, 625, bias=True) torch.nn.init.xavier_uniform_(self.fc1.weight) self.layer4 = torch.nn.Sequential( self.fc1, torch.nn.ReLU(), torch.nn.Dropout(p=self.drop_prob) ) self.fc2 = torch.nn.Linear(625, 10, bias=True) torch.nn.init.xavier_uniform_(self.fc2.weight) def forward(self, x): output = self.layer1(x) # print(output.shape) output = self.layer2(output) # print(output.shape) output = self.layer3(output) # print(output.shape) output = output.view(output.size(0), -1) output = self.layer4(output) output = self.fc2(output) return output Conv2d와 MaxPool2d을 지닌 layer를 통과하면서 output의 shape은 게속 변하게 된다. 이러한 과정에서 Fully Connected layer를 지날때는 이전 layer에서 나온 output의 shape를 알아야한다. 이때 이전 포스트에서 알아본 공식을 통해 output shape을 계산할 수 있다. 또한, forward를 진행하면서 각각의 layer를 통과한 output의 shape를 출력하면서 확인할 수도 있다. CNN 모델을 배우기 전에는 선형함수만을 통해 MNIST이미지를 학습했다면 이번 과정에서는 지금까지 배운 Dropout, weight initialization, Convolution layer 등을 사용하는 모델을 만들 수 있었다. 학습과 평가는 기존 MNIST 학습 코드와 유사하고, Full Code 링크를 통해 확인할 수 있다. Full CodeFull Code","link":"/2020/05/01/Pytorch-MNIST-CNN/"},{"title":"Pytorch_visdom으로_Loss_plot그리기","text":"Visdom에 관하여 &amp; Loss plot 그리기모두를 위한 딥러닝 - 파이토치 강의 참고 visdom을 사용하기 위해서는 서버를 실행시켜줘야 한다. jupyter notebook의 경우, New –&gt; Terminal –&gt; 실행된 터미널 창에서 다음의 명령어를 입력한다. 1&gt;&gt;&gt;python -m visdom.server pycharm을 이용한다면 파이참 하단의 terminal을 클릭해 위의 명령어를 입력하면된다. 다만, visdom의 기본 포트가 사용중이라면 에러가 발생하는데 이때는 visdom 포트변경 포스트를 확인해 해결할 수 있다. 서버가 정상적으로 실행됐다면, visdom 객체를 아래와 같이 생성하고 이를 이용해 text, 이미지, 그래프 등을 나타낼 수 있다. 12import visdomvis = visdom.Visdom() 아래에서 진행하는 예제는 실행된 local서버를 접속해 확인할 수 있다. Text 출력하기 text를 출력하기 위해서 text()를 사용한다. 1vis.text('hello world', env='main') 이때 env='main'은 실행문의 이름을 지정해 추후에 main으로 실행된 모든 창을 한번에 종료할 수 있게 해준다. Image 출력하기 image 출력은 image()를 사용한다. 랜덤한 이미지를 출력해보자. 12ex = torch.randn(3, 200, 200)vis.image(ex) 여러장의 이미지를 출력하기 위해서는 images()를 사용한다. 1vis.images(torch.Tensor(3,3,28,28)) CIFAR10의 이미지를 가져와 출력해보자. 1234cifar10 = dsets.CIFAR10(root=\"cifar10/\",train = True, transform=torchvision.transforms.ToTensor(),download=True)data = cifar10.__getitem(0)__vis.images(data[0], env='main') DataLoader로부터 여러개의 이미지를 출력할 수 있다. 1234567891011MNIST = dsets.MNIST(root=\"MNIST_data/\",train = True,transform=torchvision.transforms.ToTensor(), download=True)data_loader = torch.utils.data.DataLoader(dataset = MNIST, batch_size = 32, shuffle = False) for num, value in enumerate(data_loader): value = value[0] vis.images(value) break###&gt;&gt;&gt; batch_size만큼의 이미지가 한번에 출력된다 Line Plot 그리기 line plot은 line()을 사용한다. 임의의 y값을 가지는 그래프를 그려보자. 123Y_data = torch.randn(5)X_data = torch.Tensor([1,2,3,4,5])plt = vis.line(Y=Y_data, X=X_data) 위를 통해 plt라는 이름의 그래프가 그려지는 것을 확인할 수 있다. 만약 이때 X 범위를 설정해주지 않았다면 X의 범위는 0과 1사이에서 출력되게 된다. 이미 그려진 line plot을 업데이트도 할 수 있다. 똑같이 line()을 사용한다. 위에서 그린 plt 그래프에 하나의 값을 업데이트 해보자. 1234Y_append = torch.randn(1)X_append = torch.Tensor([6])vis.line(Y=Y_append, X=X_append, win=plt, update='append') 위와같이 업데이트할 plot의 이름을 win에 지정해주고 update='append'로 지정하면 기존의 그래프에 하나의 값이 추가된 것을 확인할 수 있다. 하나의 window에 두개의 line plot을 그릴때도 line()을 사용한다. 123456num = torch.Tensor(list(range(0,10)))num = num.view(-1,1)num = torch.cat((num,num),dim=1)print(num.shape) ###&gt;&gt;&gt; (10,2)plt = vis.line(Y=torch.randn(10,2), X = num) Line Plot의 정보 나타내기 line plot에 title과 legend를 나타내기 위해서는 dict형태의 입력값을 사용한다. 위에 그린 그래프에 title과 legned를 나타내보자. 1plt = vis.line(Y=Y_data, X=X_data, opts=dict(title='test', legend=['1번'], showlegend=True)) 두개의 line plot에도 각각의 legend를 나타낼 수 있다. 1plt = vis.line(Y=torch.randn(10,2), X=num, optes=dict(title='test', legend=['1번','2번'], showlegend=True)) 창 닫기 env='main'으로 지정한 창을 닫기 위해서 close()를 사용한다. 1vis.close(env='main') 기본적인 visdom 사용법을 알아보았으며, 이를 이용해 CNN MNIST의 코드를 조금 변형해 loss plot을 그려보자. loss plot을 그리기 위해서 line plot을 업데이트해줄 함수를 만들어주자. 123456def loss_tracker(loss_plot, loss_value, num): ### loss_plot : line plot value ### loss_value : Tensor ### num : Tensor ### return : None vis.line(X=num, Y=loss_value, win=loss_plot, update='append') 그리고 loss를 업데이트해줄 빈 line plot을 하나 만들어준다. 1loss_plot = vis.line(Y=torch.Tensor(1).zero_(), opts=dict(title='loss', legend=['loss'], showlegend=True)) 이후 모델 생성은 모두 동일하지만, train을 진행할때 위의 함수를 통해 그래프를 업데이트 해주는 부분이 추가된다. 123456789101112131415161718192021total_batch = len(data_loader)model.train()for epoch in range(epochs): avg_cost = 0 for X, Y in data_loader: X = X.to(device) Y = Y.to(device) optimizer.zero_grad() hypothesis = model(X) loss = criterion(hypothesis, Y) loss.backward() optimizer.step() avg_cost += loss / total_batch print('[Epoch: {:&gt;4}] cost = {:&gt;.9}'.format(epoch + 1, avg_cost)) loss_tracker(loss_plot, torch.Tensor([avg_cost]), torch.Tensor([epoch])) ###&gt;&gt;&gt; loss_tracker함수를 통해 그래프를 업데이트print('learning finished') 학습을 진행하면서 visdom의 line plot이 실시간으로 업데이트되는 모습을 확인할 수 있다. Full CodeFull Code - Visdom ExampleFull Code - Loss tracker","link":"/2020/05/06/Pytorch-visdom%EC%9C%BC%EB%A1%9C-Loss-plot%EA%B7%B8%EB%A6%AC%EA%B8%B0/"},{"title":"Pytorch_ImageFolder","text":"ImageFolder를 통해 데이터 가져오기모두를 위한 딥러닝 - 파이토치 강의 참고 분류된 이미지 데이터셋이 준비되어있다면 ImageFolder를 통해서 데이터를 가져올 수 있다. 예를들어, 3개의 클래스를 가지는 이미지셋을 준비했다면 다음과 같은 폴더 형태로 담아내면 된다. 123456789101112/project ㄴdata ㄴdataset_name ㄴtrain_data ㄴclass1 ㄴclass2 ㄴclass3 ㄴtest_data ㄴclass1 ㄴclass2 ㄴclass3 ㄴImageFolder_EX.py 위와 같이 각 클래스별로 데이터를 준비했다면 ImageFolder 를 이용해 데이터를 불러온다. MNIST나 CIFAR10의 데이터를 불러올 때처럼, tansform을 통해 텐서로 변환해 가져오게 된다. 1234567891011trans = transforms.Compose([ transforms.ToTensor()])train_data = torchvision.datasets.ImageFolder(root='data/custom_data/train_data', transform=trans)data_loader = DataLoader(dataset=train_data, batch_size=8, shuffle=True)test_data = torchvision.datasets.ImageFolder(root='data/custom_data/test_data', transform=trans)test_loader = DataLoader(dataset=test_data, batch_size=len(test_data)) 이후 간단한 CNN 모델을 만들어서 자신이 가진 데이터셋을 학습시킬 수 있다. 123456789101112class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(3, 6, 5), nn.ReLU(), nn.MaxPool2d(2), ) . . . # Full Code 참조 이전까지는 Pytorch에서 바로 다운로드하고 불러올 수 있는 MNIST, CIFAR10과 같은 데이터셋을 이용했다. 하지만, 이번에는 자신이 가진 고유의 데이터셋을 ImageFolder를 통해 불러오고 학습시키는 과정을 알 수 있었다. Full CodeFull Code","link":"/2020/05/07/Pytorch-ImageFolder/"},{"title":"Pytorch_About_VGG_Advance_CNN","text":"VGG 모델의 생성과정 살펴보기모두를 위한 딥러닝 - 파이토치 강의 참고 torchvision.models를 통해 VGG11~VGG19까지 이용할 수 있다. 3x224x224 input을 기준으로 만들어져 있으며, input size가 다른경우 모델을 약간 수정해서 사용할 수 있다. 이번에는 VGG에서 layer가 생성되는 과정을 살펴보려고 한다. VGG의 Source Code이며 일부만 가져와서 살펴보자. VGG 모델의 Class선언의 __init__ 부분을 살펴보자. 1234567891011121314151617181920class VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=True): super(VGG, self).__init__() self.features = features self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) if init_weights: self._initialize_weights() . . . features라는 레이어를 통과하고 pooling and linear layer를 통과하는 모습을 확인할 수 있다. 즉 처음 layer는 features부분에서 생성되는데 이는 make_layers라는 함수를 통해 이루어진다. 123456789101112131415161718192021def make_layers(cfg, batch_norm=False): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v return nn.Sequential(*layers)cfgs = { 'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],} make_layers함수를 살펴보면 cfg값에 따라서 알맞는 layer를 return하고 있다. cfgs에서 ‘A’값을 받았을때 어떻게 Convolution layer가 만들어지는지 따라가보자. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146순차적으로 'A'의 값이 for문을 통해 들어가면 layers에 어떻게 layer가 쌓이는지 누적시키며 따라가보자.(batch_norm=False)인 상황이다.1. 처음에 64 값을 받으면,layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)]가 되며 in_channels가 입력받은 64로 바뀌게된다!2. 'M' 을 입력받으면, MaxPool2d가 들어가며 in_channels값은 바뀌지 않는다.layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)]3. 다시 128 값을 받으면, 처음 64를 입력받았을때와 마찬가지로 ```Conv2d```와 ```ReLU```가 추가되며 in_channels는 128로 바뀌게된다.layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(64, 128, kernel_size=3, padding=1)nn.ReLU(inplace=True)]4. 다음으로 'M'을 입력받으면, 두번째 단계와 마찬가지로 MaxPool2d가 들어가고 in_channels는 유지된다.layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(64, 128, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)]5. 다음으로 256 값을 두번 연속해서 받으면, 첫번째 단계가 두번 실행되는것과 같다.layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(64, 128, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(128, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)conv2d= nn.Conv2d(256, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)]6. 세번째 'M'을 입력받아서 다음과 같다.layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(64, 128, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(128, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)conv2d= nn.Conv2d(256, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)]7. 위의 과정을 통해 익숙해졌으므로 [512, 512, 'M'] 입력을 한번에 살펴보자. 세 개의 입력이 들어오면 다음과 같이 layers가 쌓이게된다.layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(64, 128, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(128, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)conv2d= nn.Conv2d(256, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(256, 512, kernel_size=3, padding=1)nn.ReLU(inplace=True)conv2d= nn.Conv2d(512, 512, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)]8. 마지막으로 다시 한번 [512, 512, 'M'] 입력을 받는다. 따라서 최종적으로는 다음과 같은 layers가 생성된다.layers = [conv2d= nn.Conv2d(3, 64, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(64, 128, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(128, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)conv2d= nn.Conv2d(256, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(256, 512, kernel_size=3, padding=1)nn.ReLU(inplace=True)conv2d= nn.Conv2d(512, 512, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)conv2d= nn.Conv2d(512, 512, kernel_size=3, padding=1)nn.ReLU(inplace=True)conv2d= nn.Conv2d(512, 512, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=2, stride=2)] 위의 과정을 따라서 만들어진 모델은 VGG11을 의미하게 되는데, 다음과 같이 계산해 볼 수 있다. make_layers를 통해 만들어진 Convolution layer의 갯수는 8개였다. 그리고 Source Code를 살펴보면 이후에 fully connected layer가 3개있다. 따라서, 위의 과정으로 만들어진 모델은 8 + 3 = 11 로 VGG11 을 의미한다. 이와 마찬가지로, 나머지 모델은 다음과 같다. 123'B' --&gt; 10 + 3 = 13 --&gt; VGG13'C' --&gt; 13 + 3 = 16 --&gt; VGG16'D' --&gt; 16 + 3 = 19 --&gt; VGG19 Source Code를 따라가면서 VGG모델이 생성되는 과정을 살펴볼 수 있었고 다음에는 input size가 다를경우 VGG를 수정해서 사용하는 방법을 살펴볼 것이다. Full CodeFull Code","link":"/2020/05/12/Pytorch-About-VGG-Advance-CNN/"},{"title":"Pytorch_VGG_with_CIFAR10","text":"CIFAR10 데이터셋에 VGG 모델 적용해보기모두를 위한 딥러닝 - 파이토치 강의 참고 VGG는 3x224x224의 input을 기본으로 만들어져 있다. 따라서, 이미지의 크기가 다를경우 이미지 크기를 조정하거나 모델을 수정해서 사용할 수 있다. 이번에는 이미지 사이즈는 그대로 사용하며 VGG모델을 조금 수정해서 적용해보자. VGG의 모델이 어떻게 생성되는지는 이전 포스트 - VGG 모델 생성 살펴보기에서 확인할 수 있다. 이번에는 custom convolution layer 을 만들고 이를 maye_layers함수를 통해 생성한 후, 이를 통한 모델을 만들고자 한다. convolution layer 13개와 fully connected layer3개를 가지는 VGG13 configuration을 생성한다. 1cfg = [32,32,'M', 64,64,128,128,128,'M',256,256,256,512,512,512,'M'] 그리고, VGG Source Code에서 가져온 VGG class를 다음과 같이 일부 수정한다. 12345678910111213141516171819202122232425262728293031323334class VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=True): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Linear(512*4*4, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes) ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0) 위의 코드는 Source Code에서 아래와 같은 두 가지 사항을 변경했다 a. nn.AdaptiveAvgPool2d를 삭제했다. 왜냐하면, features layer를 통과하고 우리의 이미지 사이즈는 4x4이므로 nn.AdaptiveAvgPool2d((7, 7))을 통해 사이즈를 키워줄 필요가 없었다. b. classifier layer의 fully connected layer의 input size가 (batch size x 4 x 4) 로 수정되었다. 왜냐하면, 위와 마찬가지로 features를 통과한 이미지의 사이즈가 4x4이기 때문이다. 이처럼, input size가 다를 경우 기존의 모델을 수정해서 사용할 수 있다. 중요한점은, layer를 통과하면서 image의 size가 어떻게 변하는지를 알고 fully connected layer까지 수정해야 size에러가 발생하지 않는다는 점이다. 이를 위해서, 직접 공식을 통해 계산할 수도 있으며 아래와 같이 forward함수에서 shape를 프린트하며 확인할 수도 있다. 12345678910111213class VGG(nn.Module): . . . def forward(self, x): x = self.features(x) print(x.shape) # features layer를 통과하고 shape을 확인해보자. x = x.view(x.size(0), -1) x = self.classifier(x) return x . . . 이처럼, 사이즈가 다른 CIFAR10 이미지를 수정한 VGG모델에 넣어서 학습시켜볼 수 있었다. Full CodeFull Code","link":"/2020/05/14/Pytorch-VGG-with-CIFAR10/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"baekjoon","slug":"baekjoon","link":"/tags/baekjoon/"},{"name":"github.io","slug":"github-io","link":"/tags/github-io/"},{"name":"블로그","slug":"블로그","link":"/tags/%EB%B8%94%EB%A1%9C%EA%B7%B8/"},{"name":"Go","slug":"Go","link":"/tags/Go/"},{"name":"CS","slug":"CS","link":"/tags/CS/"},{"name":"CtCI","slug":"CtCI","link":"/tags/CtCI/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"}],"categories":[{"name":"baekjoon","slug":"baekjoon","link":"/categories/baekjoon/"},{"name":"python","slug":"baekjoon/python","link":"/categories/baekjoon/python/"},{"name":"github.io","slug":"github-io","link":"/categories/github-io/"},{"name":"Go","slug":"Go","link":"/categories/Go/"},{"name":"tutorial","slug":"Go/tutorial","link":"/categories/Go/tutorial/"},{"name":"Go","slug":"baekjoon/Go","link":"/categories/baekjoon/Go/"},{"name":"About Go","slug":"Go/About-Go","link":"/categories/Go/About-Go/"},{"name":"CS basic","slug":"CS-basic","link":"/categories/CS-basic/"},{"name":"CtCI","slug":"CS-basic/CtCI","link":"/categories/CS-basic/CtCI/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"About Python","slug":"Python/About-Python","link":"/categories/Python/About-Python/"},{"name":"Pytorch","slug":"Python/Pytorch","link":"/categories/Python/Pytorch/"}]}